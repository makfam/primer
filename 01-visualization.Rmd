# Visualization

Everyone loves visualizations. 

<!-- DK: economist theme causes the subtitle (and other labels) to bleed into the title. Can we clean this up? Use a different theme? -->

<!-- DK: Would be nice if the plot featured a statistical measure of some kind (maybe a 95% confidence interval) so that we have a motivation for learning about distributions. Probably need a very different plot. Maybe something political? Election 2020? -->

<!-- DK: Replace airquality data set with one of our primer.data sets. -->


```{r out.width = "100%", message = FALSE, echo = FALSE}
library(tidyverse)
library(ggthemes)
library(scales)
library(gapminder)

gap_p <- gapminder %>%
  filter(continent != "Oceania") %>%
  filter(year == max(year)) %>% 
  ggplot(aes(gdpPercap, lifeExp, color = continent)) +
    geom_point(alpha = 0.7) +
    geom_smooth(method = "lm", 
                formula = y ~ x,
                se = FALSE) + 
    facet_wrap(~continent, nrow = 2) +
    labs(title = "Life Expectancy and GDP per Capita",
         subtitle = "Connection between GDP and life expectancy is weakest in Africa",
         x = "GDP per Capita in USD",
         y = "Life Expectancy") +
    scale_x_log10(breaks = c(500, 5000, 50000),
                  labels = scales::dollar_format(accuracy = 1)) + 
    theme_economist() +
    theme(legend.position = "none")

gap_p
```

Once you have read this chapter, and completed the associated tutorials, you will be able to create graphics like this one with your data. Join us on the journey.


## Getting Started

This chapter focuses on **ggplot2**, one of the core packages in the **tidyverse**. To access the datasets, help pages, and functions that we will use in this chapter, load the tidyverse:

```{r} 
library(tidyverse)
```

That one line of code loads all the packages associated with the **tidyverse**, packages which you will use in almost every data analysis. It also tells you which functions from the tidyverse conflict with functions in base R or with other packages you might have loaded. (We hide these and other messages in this book because they are ugly.)


If you run this code and get the error message “there is no package called ‘tidyverse’”, you’ll need to install the package first using the code below. Then, run `library(tidyverse)` once again.


```{r, eval = FALSE}
install.packages("tidyverse")
```

If we need to be explicit about where a function (or dataset) comes from, we’ll use the special form: `package::function()`. For example, `ggplot2::ggplot()` tells you explicitly that we’re using the `ggplot()` function from the **ggplot2** package.



### How do I code in R?

Unlike other statistical software programs like Excel, SPSS, or Minitab that provide [point-and-click](https://en.wikipedia.org/wiki/Point_and_click) interfaces, R is an [interpreted language](https://en.wikipedia.org/wiki/Interpreted_language). This means you have to type in commands written in *R code*. In other words, you have to code/program in R. Note that we'll use the terms "coding" and "programming" interchangeably in this book.


If you are new to the world of coding, R, and RStudio and feel you could benefit from a more detailed introduction, we suggest you check out the short book, [*Getting Used to R, RStudio, and R Markdown*](https://rbasics.netlify.com/). @usedtor2016 includes screencast recordings that you can follow along and pause as you learn. This book also contains an introduction to R Markdown, a tool used for reproducible research in R.


### Tips

Learning to code/program is quite similar to learning a foreign language. It can be daunting and frustrating at first. However, putting in the effort goes a long way.

Here are a few useful tips to keep in mind as you learn to program:

* **Remember that computers are not actually that smart**: You may think your computer or smartphone is "smart". In reality, you have to tell a computer everything it needs to do. The instructions you give your computer can't have any mistakes in them.

* **Take the "copy, paste, and tweak" approach**: When you learn your first programming language, it is often much easier to take existing code that you know works and modify it to suit your ends. We suggest not trying to write code from memory, but rather take existing examples we have provided you, then copy, paste, and tweak them. After you start feeling more confident, you can slowly move away from this approach and write code from scratch.

* **Practice is key**:  Just as the only method to improve your foreign language skills is through lots of practice and speaking, the only method to improving your coding skills is through lots of practice. 


### Basic programming

While it is not required to be a seasoned coder/computer programmer to use R, there is still a set of basic programming concepts that new R users need to understand. We now introduce some basic programming terminology. Instead of asking you to memorize all these concepts and terminology right now, we'll guide you. To help you learn, we will always use a different font to distinguish regular text from `computer_code`. 

* *Console pane*: where you enter in commands in RStudio.

* *Running code*: the act of telling R to put our code into action. 

* *Objects*: where values are saved in R. You can *assign* values to objects and display the contents of objects. You use the assignment operator `<-` to do so.  You can choose almost any name you like for an object, as long as the name does not begin with a number or a special character like `+`, `*`, `-`, `/`, `^`, etc.

For example we can save the values of the equation below to an object named `x`. 

```{r}
x <- 4 + 3
```

Now, whenever we type `x`, the value of 4 + 3 will show:

```{r}
x
```


* *Data types*: integers, doubles/numerics, logicals, and characters. Integers are values like -1, 0, 2, 4092. Doubles or numerics are a larger set of values containing both the integers but also fractions and decimal values like -24.932 and 0.8. Logicals are either `TRUE` or `FALSE` while characters are text such as "cabbage" or "Hamilton", "The Wire is the greatest TV show ever". Note that characters are often denoted with the quotation marks around them.

* *Vectors*: a series of values. These are created using the `c()` function, where `c()` stands for "combine". 

For example, `c(2, 4, 6, 8)` creates a four element series of integer values.

```{r}
c(1, 2, 3, 4)
```


* *Factors*: *categorical data* are commonly represented in R as factors. Categorical data can also be represented as *strings*. We will go into detail about these and other variable types in Chapter 2.

* *Data frames*: rectangular spreadsheets of data. Their rows correspond to *observations* and the columns correspond to *variables* that describe the observations. Modern data frames are called *tibbles*.  

* *Boolean algebra*: `TRUE/FALSE` statements and mathematical operators such as `<` (less than), `<=` (less than or equal), and `!=` (not equal to). For example, `4 + 2 >= 3` will return `TRUE`, but `3 + 5 <= 1` will return `FALSE`. Testing for inclusion with the `%in%` operator. For example, `"B" %in% c("A", "B")` returns `TRUE` while `"C" %in% c("A", "B")` returns `FALSE`. We test for equality in R using `==`. For example, `2 + 1 == 3` compares `2 + 1` to `3` and is correct R code, while `2 + 1 = 3` will return an error. 

* *Logical operators*: `&` representing "and" as well as `|` representing "or." For example, `(2 + 1 == 3) & (2 + 1 == 4)` returns `FALSE` since both clauses are not `TRUE` (only the first clause is `TRUE`). On the other hand, `(2 + 1 == 3) | (2 + 1 == 4)` returns `TRUE` since at least one of the two clauses is `TRUE`. 

* *Functions*, also called *commands*: perform tasks in R. They take in inputs called *arguments* and return outputs. You can either manually specify a function's arguments or use the function's *default values*. 

For example, `sqrt(64)` will take the square root of 64, which is 8.

```{r}
sqrt(64)
```


* *Help files*: provide documentation for various functions and datasets. You can bring up help files by adding a `?` before the name of a function or data frame and then run this in the console. You will then be presented with a page showing the corresponding documentation. 

**Code Comments*: are text placed after a `#` symbol. Nothing will be run after a `#` symbol, which is useful if you want to write human readable comments in your code.

### Errors, warnings, and messages

R reports errors, warnings, and messages in a glaring red font, which makes it seem like it is scolding you. However, seeing red text in the console is not always bad.

R will show red text in the console pane in three different situations:

* **Errors**: Red text prefaced with "Error in…" . It will try to explain what went wrong. Generally when there's an error, the code will not run. For example, if you see `Error in ggplot(...) : could not find function "ggplot"`, it means that the `ggplot()` function is not accessible because the package that contains the function, **ggplot2**, was not loaded with `library(ggplot2)`.

* **Warnings**: Red text prefaced with "Warning:". It will try to explain why there's a warning. Generally your code will still work, but with some caveats. For example, if you create a scatterplot based on a dataset where two of the rows of data have missing entries, you will see this warning: `Warning: Removed 2 rows containing missing values (geom_point)`. R will still produce the scatterplot with all the remaining non-missing values, but it is warning you that two of the points aren't there.

* **Messages**: When the red text doesn't start with either "Error" or "Warning", it's *just a friendly message*. You'll see these messages when you load *R packages*. These are helpful diagnostic messages. They don't stop your code from working. Additionally, you'll see these messages when you install packages too using `install.packages()`.

Remember, when you see red text in the console, *don't panic*. It doesn't necessarily mean anything is wrong. Rather:

* If the text starts with "Error", figure out what's causing it. <span style="color:red">Think of errors as a red traffic light: something is wrong!</span>
* If the text starts with "Warning", figure out if it's something to worry about.  <span style="color:gold">Think of warnings as a yellow traffic light: everything is working fine, but watch out/pay attention.</span>
* Otherwise, the text is just a message.<span style="color:green">Think of messages as a green traffic light: everything is working fine and keep on going!</span>

### Examining `gapminder`

Let's put everything we've learned so far into practice and start exploring some real data! Data comes to us in a variety of formats, from pictures to text to numbers. Throughout this book, we'll focus on datasets that are saved in "spreadsheet"-type format. This is probably the most common way data are collected and saved in many fields. These "spreadsheet"-type datasets are called _data frames_ in R. Again, we will be using "data frame" and "tibble" interchangeably for this chapter.


We'll begin by exploring the `gapminder` data frame in the **gapminder** package. This dataset is an excerpt of the Gapminder data on life expectancy, GDP per capita, and population by country.


```{r}
library(gapminder)
gapminder
```

Let's unpack this output:

* `A tibble: 1,704 x 6`: A `tibble` is a specific kind of data frame in R. This particular data frame has `1,704` rows corresponding to different *observations*. 

* The tibble also has `6` columns corresponding to 6 *variables* describing each observation. `country`, `continent`, `year`,`lifeExp`, `pop`, and `gdpPercap` are the different variables of this dataset. 

* We see, by default, the top 10 rows. R is only showing the first 10 rows, since that is all that you probably want to see at first. You can see more (or fewer) rows with scrolling or using the `print()` command:

```{r}
print(gapminder, n = 15)
```

*Note:* the `n` argument tells R the number of rows you want to see. 

### Exploring data frames

There are many ways to get a feel for the data contained in a data frame such as `gapminder`. We present two functions that take the dataset as their "argument" (or input). We also include a third method for exploring one particular column of a data frame:

1. Using the `view()` function, which brings up RStudio's built-in data viewer.
1. Using the `glimpse()` function, which is included in the **dplyr** package.
1. Using the `$` "extraction operator," which is used to view a single variable/column in a data frame.

**1. `View()`**:

Run `view(gapminder)` in your console in RStudio, either by typing it or cutting-and-pasting it into the console pane. Explore this data frame in the resulting pop up viewer. You should get into the habit of viewing any data frames you encounter.

By running `view(gapminder)`, we can explore the different *variables* listed in the columns. Observe that there are many different types of variables.  Some of the variables including `year`, `lifeExp`, `pop`, `year` and `gdPercap` are *quantitative* variables. These variables are numerical in nature.  Other variables here, including `country`, `continent` are *categorical*.

If you look in the leftmost column of the `view(gapminder)` output, you will see a column of numbers. These are the row numbers of the dataset. If you glance across a row with the same number, say row 5, you can get an idea of what each row is representing. This will allow you to identify what object is being described in a given row. This is often called the *observational unit*.


**2. `glimpse()`**:

We can also explore a data frame using the `glimpse()`function. 

```{r}
glimpse(gapminder)
```

Observe that `glimpse()` will give you the first few entries of each variable in a row after the variable name.  In addition, the *data type* of the variable is given immediately after each variable's name inside `< >`. 

Here, `dbl` refers to "double", which is computer coding terminology for quantitative/numerical variables. `int` refers to "integer" and is another data type that also represents quantitative/numerical variables.`fct` refers to "factor" and describes a variable that is nominal, or in this case the `country` and `continent` variables.

Not mentioned here is `chr`, which refers to "character", which is computer terminology for text data.

**3. `$` operator**

Lastly, the `$` operator allows us to extract and then explore a single variable within a data frame.

```{r}
gapminder$year
```

We used the `$` operator to extract only the `year` variable and return it as a vector. We'll only be occasionally exploring data frames using the `$` operator. Mostly, we favor the `view()` and `glimpse()` functions.


## Basic Plots

We begin the development of your data science toolbox with data visualization. By visualizing data, we gain valuable insights we couldn't initially obtain from just looking at the raw data values. Graphics/plots/charts provide a nice way to explore the patterns in data, such as the presence of *outliers*, *distributions* of individual variables, and *relationships* between groups of variables. We'll use the **ggplot2** package, as it provides an easy way to customize your plots. 

We can break a graphic into the following three essential components:

* `data`: the dataset containing the variables of interest.
* `geom`: the geometric object in question. This refers to the type of object we want to observe in a plot (scatterplot, line graph, etc.).
* `aes`: aesthetic attributes of the geometric object. This is where we tell R what variables should be on the x and y axis. Additionally, color and size are additional attributes we will explore later on. Aesthetic attributes are *mapped* to variables in the dataset.

Let's look at an example of a basic scatterplot. 

```{r}
ggplot(data = gapminder, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp)) + 
  geom_point()
```

Notice how the three components( i.e.`data`, `geom`, and `aes`) are specified in the `ggplot()` function.

Once we've specified these components, we then add *layers* to the plot using the `+` sign. The most essential layer to add to a plot is the layer that specifies which type of `geom`etric object we want the plot to involve: points, lines, bars, and others. In our graph above, the geom we used is `geom_point()`. This tells R we want our graph to include points (i.e. a scatterplot). 

If you look at the code above, you will notice the `+` sign comes at the end of the code line and not at the beginning. When adding layers to a plot, start a new line after the `+` so that the code for each layer is on a new line.

Let's now dive deeper into `geom_point()`.  

### `geom_point()`

*Scatterplots*, also called *bivariate plots*, allow you to visualize the *relationship* between two numerical variables. 

Recall our scatterplot from above.

```{r}
ggplot(data = gapminder, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp)) + 
  geom_point()
```

Let's break down this code, piece-by-piece.

* The `data` argument is set to `gapminder` via `data = gapminder`.\

* The `aes`thetic `mapping` by set via `mapping = aes(x = gdpPercap, y = lifeExp)`. Here, we map `gdpPercap` to the `x` axis and `lifeExp` maps to the `y` axis.

* The `geom`etric object is specified to `geom_point()`, telling R we want a scatterplot. We added a layer using the `+` sign.

In the graphic above, notice that a *positive relationship* exists between `lifeExp` and `gdpPercap`: as gdp per capita increases, life expectancy tends to also increase. If we do not specify the `geom`etric object, we have a blank plot like such:

```{r}
ggplot(data = gapminder, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp)) 
```

#### More Aesthetics 

In addition to mapping varaibles to the `x` and `y` axises, we can also map variables to `color`. They can be very useful tools. 

For example, because the data plotted on our scatterplot is from more than one continent, it can be hard to know which point of data corresponds to which continent. Let's change that by setting the `color` argument of `aes()` to the variable `continent`.

```{r}
ggplot(data = gapminder, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp,
                     color = continent)) +
geom_point()
```

Now we have a slightly better understanding of our data. However, note that the points of the scatterplot are all overlapping, so we do not know if some colors are being covered by others.

<!-- BG: size here? -->

#### Titles and Labels

We use the function `labs()` to add a plot title, axis labels, subtitles, and captions to our graph. You will have noticed that by default R simply uses the names of variables for axes and legends. Let's now add a title and axis labels to our plot.

```{r}
ggplot(data = gapminder, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp)) + 
  geom_point() + 
  labs(title = "The Relationship Between Life
               Expectancy and GDP Percapita",
            x = "GDP Percapita",
            y= "Life Expectancy")
```

Note that like `geom`s, we add a layer  when including the function `labs()`to our plot. Note that it is up to us, which or how many of the arguments in `labs()` we use.


Let's now take a tour of some of the more useful geoms.


### `geom_jitter()`

Our scatterplot from the previous section has a large mass of points. This makes it hard to tell the true number of points that are plotted. This is the result of a phenomenon called *overplotting*, where points are being plotted on top of each other over and over again. There are two methods we can use to address overplotting.

1. Adjust the transparency of the points 
1. Add a little random "jitter" or random "nudges" to each of the points

**Method 1: Changing the transparency**

We can change the transparency/opacity of the points by using the `alpha` argument within `geom_point()`. The `alpha` argument can be set to any value between `0` and `1`, where `0` sets the points to be 100% transparent and `1` sets the points to be 100% opaque. By default, `alpha` is set to `1`. In other words, if we don't explicitly set an `alpha` value, R will use `alpha = 1`.


Let's now add an `alpha` argument to our scatterplot. 

```{r}
ggplot(data = gapminder, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp))  + 
      labs(title= "The Relationship Between Life Expectancy 
                    and GDP Percapita",
                    x = "GDP Percapita",
                    y= "Life Expectancy") +
      geom_point(alpha = .2)
```

Note that there is no `aes()` surrounding `alpha = 0.2`. This is because we are not mapping a variable to an aesthetic attribute, but rather merely changing the default setting of `alpha`. In fact, you'll receive an error if you try to change the second line to read `geom_point(aes(alpha = 0.2))`.

**Method 2: Jittering the points**

We can also decide to jitter the points on the plot. We do this by replacing `geom_point()` with `geom_jitter()`. Keep in mind that jittering is strictly a visualization tool; even after creating a jittered scatterplot, the original values saved in the data frame remain unchanged. 

In order to specify how much jitter to add, we use the `width` and `height` arguments to `geom_jitter()`. This corresponds to how hard you'd like to shake the plot in horizontal x-axis units and vertical y-axis units, respectively. It is important to add just enough jitter to break any overlap in points, but not to the extent where you alter the original pattern in points.

```{r}
ggplot(data = gapminder, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp))  + 
      labs(title= "The Relationship Between Life Expectancy 
                    and GDP Percapita",
                    x = "GDP Percapita",
                    y= "Life Expectancy") +
      geom_point(alpha = .2)
```

As can be seen in the resulting plot, jittering doesn't really provide much new insight. It can be argued that changing the transparency of the points by setting `alpha` proved more effective. When deciding whether to jitter a scatterplot or use the `alpha` argument to `geom_point()`, know that there is no single right answer. We suggest you play around with both methods to see which one better emphasizes the point you are trying to make. 

### `geom_line()`

Linegraphs show the relationship between two numerical variables when the variable on the x-axis, also called the *explanatory* variable, is of a sequential nature. In other words, there is an inherent ordering to the variable. 

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("https://imgs.xkcd.com/comics/decline.png")
```

The most common examples of linegraphs have some notion of time on the x-axis: hours, days, weeks, years, etc. Since time is sequential, we connect consecutive observations of the variable on the y-axis with a line. Linegraphs that have some notion of time on the x-axis are also called *time series* plots. 

<!-- BG: need data set to show graph here. I tried it with gapminder and it didn't work well -->




### `geom_histogram()`

A histogram is a plot that visualizes the *distribution* of a numerical value as follows:

1. We first cut up the x-axis into a series of *bins*, where each bin represents a range of values. 
1. For each bin, we count the number of observations that fall in the range corresponding to that bin.
1. Then for each bin, we draw a bar whose height marks the corresponding count.

Let's consider the `lifeExp` variable of the the `gapminder` data frame. Unlike with the linegraphs, let's say we don't care about its relationship with time, but rather we only care about how the values of `lifeExp` *distribute*. 


Here is our code to create a histogram:

```{r}
ggplot(data = gapminder, 
       mapping = aes(x = lifeExp)) +
  geom_histogram()
 
```

Unlike scatterplots and linegraphs, there is now only one variable being mapped in `aes()`. Here, that variable is `lifeExp`. The y-aesthetic of a histogram, the count of the observations in each bin, gets computed for you automatically. Furthermore, the geometric object layer is now a `geom_histogram()`. After running the following code, you'll see the histogram as well as warning messages. 

* The message is telling us that the histogram was constructed using `bins = 30` for 30 equally spaced bins. This is the default value; unless you override this default number of bins with a number you specify, R will choose 30 by default. Because this is an important aspect of making a histogram, R insists on informing you with this message. You make this message go away by specifying the `bin` number yourself, as you should always do.

It's hard to get a sense for which range of life expectancy is spanned by each bin; everything is one giant amorphous blob. So let's add white vertical borders demarcating the bins by adding a `color` argument to `geom_histogram()`. We set `color `to "white" in  `geom_histogram()` to create a white outline between bins.

```{r}
ggplot(data = gapminder, 
       mapping = aes(x = lifeExp)) +
  geom_histogram(color = "white")
```

We can also use the `fill` argument to change the color of the actual bins. Let's set `fill` to "steelblue". 

```{r}
ggplot(data = gapminder, 
       mapping = aes(x = lifeExp)) +
  geom_histogram(bins = 20,
                 color = "white", 
                 fill = "steelblue")
```

We can also adjust the number of bins in our histogram in one of two ways:

1. By adjusting the number of bins via the `bins` argument to `geom_histogram()`. 

2. By adjusting the width of the bins via the `binwidth` argument to `geom_histogram()`. 


Using the second method, instead of specifying the number of bins, we specify the width of the bins by using the `binwidth` argument in the `geom_histogram()` layer. For example, let's set the width of each bin to be 5.

```{r}
ggplot(data = gapminder,
       mapping = aes(x = lifeExp)) +
  geom_histogram(binwidth = 5, 
                 color = "white",
                 fill = "steelblue")
```


### `geom_bar()` 

Similarly to histograms, `geom_bar()` visualizes the distribution of a categorical variable. This is a simpler task, as we are simply counting different categories within a categorical variable, also known as the *levels* of the categorical variable. Often the best way to visualize these different counts, also known as *frequencies*, is with barplots.

Run the following code that manually creates a data frame representing a collection of fruit: 3 apples and 2 oranges. Notice how `fruits` lists the fruit individually.

```{r}
my_fruit <- tibble(fruit = c("apple", "apple", "orange", 
                           "apple", "orange"))

my_fruit
```

Using the `my_fruit` data frame where all 5 fruits are listed individually in 5 rows, we map the `fruit` variable to the x-position aesthetic and add a `geom_bar()` layer:

```{r}
ggplot(data = my_fruit, 
       mapping = aes(x = fruit)) +
  geom_bar()
```


### `geom_col()` 

`geom_col()` is very similar to `geom_bar()`, except that `geom_col()` requires you to calculate the number of rows in each category ahead of time. `geom_bar()` does the calculation for you.

<!-- DK: Don't use tibble! -->

```{r}
tibble(fruit = c("apple", "orange", "lemon"),
       number = c(3, 2, 5)) %>% 
  ggplot(aes(x = fruit, y = number)) +
    geom_col()
```

You will learn about the `tibble()` function in the next chapter.


Note how we must supply both an `x` and `y` aesthetic for `geom_col()`. We need a variable for the category, as with `geom_bar()` and for the count of entries for each value.


#### No pie charts!

One of the most common plots used to visualize the distribution of categorical data is the pie chart. While they may seem harmless enough, pie charts actually present a problem in that humans are unable to judge angles well. @robbins2013 argues that we overestimate angles greater than 90 degrees and we underestimate angles less than 90 degrees. In other words, it is difficult for us to determine the relative size of one piece of the pie compared to another.  

While pie charts present information in a way such that comparisons must be made by comparing angles, barplots are more effective because they present the information in a way such that comparisons between categories can be made with single horizontal lines.


#### Two categorical variables

Barplots are a very common way to visualize the frequency of different categories, or levels, of a single categorical variable. Another use of barplots is to visualize the *joint* distribution of two categorical variables at the same time. 

Let's look at a different dataset `mpg`, which contains a subset of the fuel economy data that the EPA released every year between 1999 and 2008. Each observation is a different car.

Let's examine the *joint* distribution of two different types of manufacturers' cars by `class` as well as `model`. In other words, the number of cars for each `class` and `model` combination. 

The code below creates a barplot of `class` frequency.

```{r}
ggplot(data = mpg, 
       mapping = aes(x = class)) + 
  geom_bar()
```

Let's now map the additional variable `model` by adding a `fill = model` inside the `aes()` aesthetic mapping. Recall the `fill` aesthetic corresponds to the color used to fill the bars.

```{r}
ggplot(data = mpg, 
       mapping = aes(x = class, 
                     fill = model)) +
  geom_bar()
```

This is an example of a *stacked barplot*.  While simple to make, in certain aspects it is not ideal. For example, it is difficult to compare the heights of the different colors between the bars, corresponding to comparing the number of cars from each model between classes.

An alternative to stacked barplots are *side-by-side barplots*, also known as *dodged barplots*. The code to create a side-by-side barplot  includes a `position = "dodge"` argument added inside `geom_bar()`. In other words, we are overriding the default barplot type, which is a *stacked* barplot, and specifying it to be a side-by-side barplot instead.


```{r}
ggplot(data = mpg, 
       mapping = aes(x = class, 
                     fill = model)) +
  geom_bar(position = "dodge")
```

Note the width of the bars vary among one another. We can make one tweak to the `position` argument to get them to be the same size in terms of width as the other bars by using the more robust `position_dodge()` function.

```{r}
ggplot(data = mpg, 
       mapping = aes(x = class, 
                     fill = model)) +
  geom_bar(position = position_dodge(preserve = "single"))
```





### `geom_smooth()`

We can add trend lines to the plots we create using the `geom_smooth()` function. 

<!-- DK: Will handle geom_smooth().explain method, se, and formula -->

Recall the following scatterplot from the `geom_point()` section. 

```{r}
ggplot(data = gapminder, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp)) + 
  geom_point()
```

We can add a trend line to our graph by adding the layer `geom_smooth()`. Including trend lines allow us to visualize the patterns in the presence of overplotting.

```{r}
ggplot(data = gapminder, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp)) + 
  geom_point() +
  geom_smooth()
```

Do you see the message R gives us? R is telling us that we need to specify the `method` and `formula` argument. 

Let's first deal with the `method` argument. Use the `geom_smooth()` function in combination with the argument `method = lm`, where `lm` stands for linear model.

```{r}
ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + 
  geom_point() +
  geom_smooth(method = lm)
```

Notice R continues to give us a message that we still need to include and specify the `formula` argument. Let's set the `formula` to `y ~ x` within `geom_smooth()`.

```{r}
ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + 
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x)
```

Notice the gray section surrounding both sides of both of the lines we plotted. This area is called the confidence interval, which is set to a 95% confidence interval by default. Therefore, using the 95% default confidence interval, we can be 95% certain that predictions for the model used in each plot fall within the gray area.

If we want to remove the gray area, or the confidence interval, we can set `se = FALSE` within the `geom_smooth()` function.

```{r}
ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + 
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE)
```

### `geom_density()`

Recall the histogram we plotted in the `geom_histogram()` section.

```{r}
ggplot(data = gapminder, mapping = aes(x = lifeExp)) +
  geom_histogram(bins = 20)
```

We can change `geom_histogram()` to `geom_density()` to make a density plot, which is a smoothed version of the histogram. This is a useful alternative to the histogram that displays continuous data in a smooth distribution.

```{r}
ggplot(data = gapminder, mapping = aes(x = lifeExp)) +
  geom_density()
```

<!-- DK: Other stuff to finish this section with? -->


## The Tidyverse

Let's go over some important points about specifying the arguments (i.e., inputs) to functions. Run the following two segments of code:

```{r message = FALSE}
ggplot(data = gapminder, 
       mapping = aes(x = lifeExp)) +
geom_histogram(color = "white")
 
```

```{r message = FALSE}
ggplot(data = gapminder, 
       mapping = aes(x = lifeExp)) +
geom_histogram(color = "white")
```

Both code segments create the same barplot, even though in the second segment we omitted the `data = ` and `mapping = ` code argument names. This is because the `ggplot()` function by default assumes that the `data` argument comes first and the `mapping` argument comes second. As long as you specify the data frame in question first and the `aes()` mapping second, you do not need to include the explicit statement of the argument names `data = ` and `mapping = `. 

Going forward for the rest of this book, all `ggplot()` code will be like the second segment: with the `data = ` and `mapping = ` explicit naming of the argument omitted with the default ordering of arguments respected. We'll do this for brevity's sake; it's common to see this style when reviewing other R users' code.


### Data Wrangling 

We can't use all the beautiful plots that we learned in the previous chapter until we have "wrangled" the data into a convenient shape. In this chapter, we'll introduce a series of functions from the **tidyverse** collection of packages which help with wrangling, and everything else we need to do to work with data. Such functions include:

* `filter()`the data frame's existing rows to only pick out a subset of them.

* `select()` specific variable columns in a data set. 

* `arrange()` its rows. You can organize your data in ascending or descending order. 

* `group_by()` its rows. This function assigns different rows to be part of the same *group*. This allows data statistics to be calculated for each group *separately*. You will see this function used with `summarize()`.

*`mutate()` its existing columns/variables to create new ones.

*`summarize()` the data. It creates a new data frame comprised of summary statistics for one (or more) rows for each grouped variable. 

Notice how we used `computer_code` font to describe the actions we want to take on our data frames. This is because the **dplyr** package, one of the packages in the **tidyverse**, has intuitively verb-named functions that are easy to remember. 

<!-- BG: stopped here -->

### The pipe operator: `%>%`

Before we start data wrangling, let's first introduce a nifty tool that gets loaded with the **dplyr** package included in the tidyverse: the pipe operator `%>%`. The pipe operator allows us to combine multiple operations in R into a single sequential *chain* of actions.

Recall our code for our scatterplot.

```{r}
ggplot(data = gapminder, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp)) + 
  geom_point()
```

Much like how the `+` sign has to come at the end of lines when constructing plots, the pipe operator `%>%` has to come at the end of lines as well. Without the `+` at the end of the first row, your computer would not know to continue onto the second. 


The code below filters the `gapminder` dataset to only show about Asia, opposed to having data on all continents.

```{r}
gapminder %>% 
  filter(continent == "Asia")
```

A single *chain* of data wrangling operations is formed here by combining verb-named functions into a single sequence using the pipe operator `%>%`. 



### `filter()` rows

```{r, echo = FALSE, out.width = "100%", fig.cap = "Diagram of filter() rows operation."}
knitr::include_graphics("01-visualization/images/filter.png")
```

The `filter()` function here works much like the "Filter" option in Microsoft Excel; it allows you to specify criteria about the values of a variable in your dataset and then filters out only the rows that match that criteria.

Let's revisit the code from the previous section.

```{r}
gapminder %>% 
  filter(continent == "Asia")
```

The result of using `filter()` will be a transformed/modified data frame that you want, in this case, only data from Asia. When we alter our data, it can be a good idea to save the result in a new data frame by using the `<-` assignment operator.

```{r}
gapminder_asia <- gapminder %>% 
  filter(continent == "Asia")
```

Let's break down the code here: 

Here we assigned our new data to an object named `gapminder_Asia` via `gapminder_Asia <-`. Because we assigned this modified data frame to `gapminder_Asia`, it is a separate entity from the initial `gapminder` data frame. If, however, we had written the code as `gapminder <- gapminder` you would have overwritten the previous data frame, and the original `gapminder` data from the **gapminder* package would have to be re-installed to access it again.

We take the `gapminder` data frame *then* `filter()` the data frame so that only those where the `continent` equals `"Asia"` are included. We test for equality using the double equal sign `==` and not a single equal sign `=`. In other words `filter(continent = "Asia")` will yield an error. This is a convention across many programming languages. If you are new to coding, you'll probably forget to use the double equal sign `==` a few times before you get the hang of it.

You can use other operators beyond just the `==` operator that tests for equality:

- `>` corresponds to "greater than"
- `<` corresponds to "less than"
- `>=` corresponds to "greater than or equal to"
- `<=` corresponds to "less than or equal to"
- `!=` corresponds to "not equal to." The `!` is used in many programming languages to indicate "not."

Furthermore, you can combine multiple criteria using operators that make comparisons:

- `|` corresponds to "or"
- `&` corresponds to "and"



Let's see see some of these in action. First, let's explore a new data set called `airquality` . The dataset includes daily airquality measurements in New York, May to September 1973. Let's filter `airquality` for all rows that contain data from months May to December. We can do this by using `>=` operator. Run the following code in your console:

```{r}
airquality %>% 
  filter(Month >= 5)
```


Let's now use the `mpg` tibble to explore `&` and `|` operators. Let's say we were interested in cars whose class is compact AND the manufacturer is Audi OR Volkswagen. We would use the following code:

```{r}
mpg %>% 
  filter(class == "compact" &
        (manufacturer == "audi" | manufacturer == "volkswagen"))
```

**Note:** We can often skip the use of `&` and just separate our conditions with a comma. The previous code will return the identical output as above.

```{r}
mpg %>% 
  filter(class == "compact",
        (manufacturer == "audi" | manufacturer == "volkswagen"))
```

Let's present another example that uses the `!` "not" operator to pick rows that *don't* match a criteria. As mentioned earlier, the `!` can be read as "not." Here we are filtering rows corresponding to cars that are not manufactured by Audi or Volkswagen.

```{r}
mpg %>% 
  filter(!(manufacturer == "audi" | manufacturer == "volkswagen"))
```

Note the careful use of parentheses around the `(manufacturer == "audi" | manufacturer == "volkswagen")`. If we didn't use parentheses as follows:

```{r}
mpg %>%
  filter(!manufacturer == "audi" | manufacturer == "volkswagen")
```

The code would read as cars NOT manufactured by Audi or cars that are actually manufactured by Volkswagen. In other words, "yes" to Audi or "no" to Volkswagen.

If we decided to include more manufacturers, it would be unwieldy to write. A slightly shorter approach uses the `%in%` operator along with the `c()` function. Recall that the `c()` function "combines" or "concatenates" values into a single *vector* of values.

```{r}
mpg %>% 
  filter(manufacturer %in% c("audi","volkswagen","toyota"))
```

What this code is doing is filtering `mpg` for all cars where `manufacturer` is in the vector: `c("audi","volkswagen","toyota")`. Remember we could have used `==` here, but using `%in%` along with `c()` takes much less energy to code.

As a final note, we recommend that `filter()` should often be among the first verbs you consider applying to your data. This cleans your dataset to only those rows you care about.



### `select` variables

```{r, echo = FALSE, out.width = "100%", fig.cap = "Diagram of select() columns."}
knitr::include_graphics("01-visualization/images/select.png")
```

Using the `filter()` function we were able to pick out specific *rows* from the dataset. The `select()` function allows R users to pick specific *columns* (or variables) instead.

We've seen that the `gapminder` data frame in the `gapminder` package contains 6 different variables. You can identify the names of these 6 variables by running the `glimpse()` function from the **dplyr** package:

```{r}
glimpse(gapminder)
```

However, say you only need 2 of these 6 variables, say `continent` and `lifeExp`. You can `select()`  these two variables:

```{r}
gapminder %>% 
  select(continent,lifeExp)
```

This function makes it easier to explore large datasets since it allows us to limit the scope to only those variables we care most about. For example, if we `select()` only a smaller number of variables, it will make viewing the dataset in RStudio's spreadsheet viewer more digestible. 

Let's say instead you want to drop, or de-select, certain variables. For example, consider the variable `year` in the `gapminder` data frame. Say you want to look at all the variables except year. You can remove this variable from the data frame. We can deselect `year` by using the `-` sign:

```{r}
gapminder %>% 
  select(-year)
```

Another way of selecting columns/variables is by specifying a range of columns using the `:` operator. 

```{r}
gapminder %>% 
  select(country:lifeExp)
```

This will `select()` all columns between the two specified variables. By running this code, we select `country`, `continent`, `year`, and `lifeExp`.

The `select()` function can also be used to rearrange columns when used with the `everything()` helper function.  For example, suppose we want the `continent`, `country` variables to appear immediately after the `year` variable, while not discarding the rest of the variables. In the following code, `everything()` will pick up all remaining variables: 

```{r}
gapminder %>% 
  select(year, continent, country, everything())
```

Lastly, the helper functions `starts_with()`, `ends_with()`, and `contains()` can be used to select variables/columns that match those conditions. Examples:

```{r}
gapminder %>% 
  select(starts_with("c"))
```

```{r}
gapminder %>% 
  select(ends_with("p"))
```

```{r}
gapminder %>% 
  select(contains("t"))
```

### `slice()` and `pull()` and `[]`

`slice()` and `pull()` are additional functions that you can use to pick out specific rows or columns within a data frame.

Using `slice()` gives us specific rows from the `gapminder` tibble:

```{r}
slice(gapminder, 2:5)
```

Unlike `filter()`, `slice()` relies on numeric order of the data. 

`pull()` grabs out a variable as a vector, rather than leaving it within a tibble, as `select()` does:

```{r}
slice(gapminder, 2:5) %>% 
  pull(lifeExp)
```


### `arrange()` rows

One of the most commonly performed data wrangling tasks is to sort a data frame's rows in the alphanumeric order of one of the variables. Unlike `filter()` or `select()`, `arrange()` does not remove any rows or columns from the data frame. Instead, the **dplyr** package's `arrange()` function allows us to sort/reorder a data frame's rows according to the values of the specified variable.

Suppose we are interested in finding which top 5 countries that have the highest life expectancies in the `gapminder` dataset.


We would first want to `select()` some pertinent variables to make the data more easy to read.

```{r}
gapminder %>% 
  select(country, year, lifeExp)
```

It would be tedious to go through all 1,704 rows trying to find the top 5 years. This is where we can use `arrange()` to organize our data.

```{r}
gapminder %>% 
  select(country, year, lifeExp) %>% 
  arrange(lifeExp)
```

Notice how the life expectancy column is sorted with the smallest values to the largest. This is because `arrange()` always returns rows sorted in ascending order by default. To switch the ordering to be in "descending" order instead, we use the `desc()` function like so:

```{r}
gapminder %>% 
  select(country, year, lifeExp) %>% 
  arrange(desc(lifeExp))
```
 
As you can see, we now have our countries of interest. 

Be aware that when you use the `desc()` helper function with a character variable, will sort the destinations reverse alphabetically. 

```{r}
gapminder %>%
  arrange(continent)
```



### `mutate()` 

```{r, echo = FALSE, out.width = "100%", fig.cap = "Diagram of mutate() columns."}
knitr::include_graphics("01-visualization/images/mutate.png")
```

Another common transformation of data is to create/compute new variables based on existing ones. In other words, we want to make a new column using already existing data. We can use the `mutate()` function from the **dplyr** package, which takes existing variables and mutates them to create new ones. 

Let's create a new variable using the `trains` dataset form the **primer.data** package. The `trains` dataset contains data for attitudes toward immigration-related policies, both before and after an experiment which randomly exposed a treated group to Spanish-speakers on a Boston commuter train platform.

Let's first load the **primer.data** package so we can`glimpse()` the data set `trains`.

```{r}
library(primer.data)
glimpse(trains)
```

Notice that the `income` variable is written out in full value. Let's use `mutate()` to create a new variable where income is written in thousands. The code would look like the following:

```{r}
trains %>% 
  mutate(income_in_thousands = (income) / 1000)
```

Notice that we have a newly created column at the far right-hand side of our data set named `income_in_thousands`. 

When creating new variables we can also *overwrite* the original data frames like `trains` like so:

```{r}
trains <- trains %>% 
  mutate(income_in_thousands = (income) / 1000)
```

Why overwrite a data frame `trains` instead of assigning the result to a new data frame like `trains_new`? As a rough rule of thumb, as long as you are not losing original information that you might need later, it's acceptable practice to overwrite existing data frames with updated ones, as we did here. On the other hand, why did we not overwrite the variable `income`, but instead created a new variable called `income_in_thousands`?  Because if we did this, we would have erased the original information contained in `income` and would not be able to use them in the future. 


#### `ifelse()`

<!-- Discuss case_when()? -->

<!-- DK: Edit these lines from R4DS. Are there other functions which we should mention here? Do something sensible with group_by. To get `_` to work in caption title. Found at https://github.com/rstudio/bookdown/issues/209  -->

`ifelse()` has three arguments. The first argument `test` should be a logical vector. The result will contain the value of the second argument, `yes`, when test is TRUE, and the value of the third argument, `no`, when it is FALSE. 

Imagine that we want to create a new variable `compact`, which is TRUE when the class of the car is "compact" and FALSE otherwise. 

```{r}
mpg %>% 
  select(manufacturer, model, class) %>% 
  mutate(compact = ifelse(class == "compact", TRUE, FALSE))
```

Alternatively to `ifelse()`, use `dplyr::case_when()`. `case_when()` is particularly useful inside mutate when you want to create a new variable that relies on a complex combination of existing variables. Note that there is a more robust version of `ifelse()` in **dplyr**: `if_else()`. This works exactly the same as the standard version but is somewhat more robust. 


### `summarize()`

A musical interlude inspired by the Tidyverse:

```{r, echo = FALSE}
knitr::include_app("https://www.youtube.com/watch?v=p8Py9C8iq2s")
```

<!-- DK: Don't use this data set. -->

The next common task when working with data frames is to compute *summary statistics*. Summary statistics are single numerical values that summarize a large number of values. Commonly known examples of summary statistics include the *mean* (also called the average) and the *median* (the middle value). Other examples of summary statistics that might not immediately come to mind include the *sum*, the smallest value also called the *minimum*, the largest value also called the *maximum*, and the *standard deviation*.


The function `sumamary()` offers an array of summary statistics for each of the columns of the dataset.

```{r}
summary(airquality)
```

The function `summarize()` (or alternatively `summarise()`) allows us to calculate these statistics on individual columns of the dataset. 

Let's calculate summary statistics of the `Temp` variable in the `airquality` data frame. Let's calculate the mean and standard deviation. To compute these summary statistics, we need the `mean()` and `sd()` *summary functions* in R.

Recall the output of the `summary()` function. 

```{r}
airquality %>% 
  summarize(mean = mean(Temp), 
            std_dev = sd(Temp))
```

The `mean()` and `sd()` summary functions go inside the `summarize()`  function from the **dplyr** package. Note you can also use the British English spelling of `summarise()`. The `summarize()` function takes in a data frame and returns a data frame with only one row corresponding to the summary statistics. 



### Statistics for distributions

A variable in a tibble is a column, a vector of values. We sometimes refer to this vector as a "distribution." This is somewhat sloppy in that a distribution can be many things, most commonly a mathematical formula. But, strictly speaking, a "frequency distribution" or an "empirical distribution" is a list of values, so this usage is not unreasonable.

*There are two distinct concepts: a distribution and a set values drawn from that distribution.*  But, in everyday use, we apply "distribution" to both. When given a distribution (meaning a vector of numbers), we often use `geom_histogram()` or `geom_density()` to look at it. But, sometimes, we don't want to look at the whole thing. We just want some summary measures which report the key aspects of the distribution. The two most important attributes of a distribution are its *center* and its *variation* around that center.

We use `summarize()` to calculate statistics for a variable, a column, a vector of values or a distribution. Note the language sloppiness. For the purposes of this book, "variable," "column," "vector," and "distribution" all mean the same thing. Popular statistical functions include: `mean()`, `median()`, `min()`, `max()`, `n()` and `sum()`. Functions which may be new to you include three measures of the "spread" of a distribution: `sd()` (the standard deviation), `mad()` (the scaled median absolute deviation) and `quantile()` (use to calculate an *interval* which includes a specified proportion of the values). 

**`mean()`**

The mean, or average, is the most commonly reported measure of the center of a distribution.  The mean is the sum of all of the data elements divided by how many elements there are. If we have $N$ data points, the mean is given by: 

$$\bar{x} = \frac{x_1 + x_2 + \cdots + x_N}{N}$$

**`median()`**

<!-- DK: Use math formula, like this one: https://math.stackexchange.com/questions/1097546/what-is-the-general-formula-for-calculating-the-median -->

The median is another commonly reported measure of the center of a distribution, calculated by first sorting the vector of values from smallest to largest. The middle element in the sorted list is the *median*.  If the middle falls between two values, then the median is the mean of those two middle values. The median and the mean are the two most common measures of the *center* of a distribution. The median is more stable, less affected by outliers. There is no widely accepted symbol for the median, although $\tilde{x}$ is not uncommon.

**`sd()`**

The standard deviation* (*sd*) of a distribution is a measure of its variation around the mean.  

$$\text{sd} = \sqrt{\frac{(x_1 - \bar{x})^2 + (x_2 - \bar{x})^2 + \cdots + (x_n - \bar{x})^2}{n - 1}}$$
<!-- DK: More detail. How much does it cover? Plot a not-too-weirdish distribution and show two sd's on it. -->


**`mad()`**

The scaled median absolute deviation (*mad*) is a measure of variation around the median.  It is not as popular as the standard deviation. The formula for calculating *mad* is a bit mysterious.


$$\text{mad} = 1.4826 \times \text{median}(abs(x - \tilde{x}))$$

The basic idea for both *sd* and *mad* is that we need a measure of variation around the center of the distribution. *sd* uses the mean, $\bar{x}$, as its estimate of the center while *mad* uses the median, $\tilde{x}$. Because *mad* uses the absolute difference, as opposed to the squared difference, it is more robust to outliers. The 1.4826 multiplier causes the *mad* and the *sd* to be identical in the (important) case of standard normal distributions, a topic we will cover in Chapter \@ref(wrangling).

<!-- DK: Could have more conversation. -->

<!-- DK: These formulas are inconsistent, with one using vectors. Should make these consistent, perhaps even give formula for median. -->


**`quantile()`**

The *quantile* of a distribution is the value of that distribution which occupies a specific percentile location in the sorted list of values. 

The 5th percentile distribution is the point below which 5% of the data falls. The 95th percentile is, similarly, the point below which 95% of the data falls. The 50th percentile, the median, splits the data into two separate, and equal, parts. The minimum is at the 0th percentile. The maximum is at the 100th percentile.

Let's take a look at the `poverty` variable in the `kenya` tibble from the **primer.data** package. `poverty` is the percentage of residents in each community with incomes below the poverty line. Let's first confirm that connections between `quantile()` and simpler functions.

```{r}
c(min(kenya$poverty), median(kenya$poverty), max(kenya$poverty))
quantile(kenya$poverty, probs = c(0, 0.5, 1))
```

The `probs` argument allows us to specify the percentile(s) we want. Two of the most important percentiles are the 2.5th and 97.5th because they define the *95% interval*, a central range which includes 95% of the values.

```{r}
quantile(kenya$poverty, probs = c(0.025, 0.975))
```

The interval between these two percentiles includes 95% of all the values in the distribution. Depending on the context, this interval is sometimes called a "confidence interval" or "uncertainty interval" or "compatibility interval." Different percentile ranges create intervals of different widths. 

An interesting fact is that, most of the time, the 95% confidence interval is, roughly, the same as the center of the distribution $\pm$ two times the standard deviation.

In distributions without major outliers, the mean/median and sd/mad will be very similar to each other.

```{r}
kenya %>% 
  summarise(mean = mean(poverty),
            median = median(poverty),
            sd = sd(poverty),
            mad = mad(poverty))
```

It follows that intervals defined with them will be similar.

```{r}
kenya %>% 
  summarise(interval_1_bottom = mean(poverty) - 2 * sd(poverty),
            interval_1_top = mean(poverty) + 2 * sd(poverty),
            interval_2_bottom = median(poverty) - 2 * mad(poverty),
            interval_2_top = median(poverty) + 2 * mad(poverty))
```

These intervals are similar to each other and similar to the 95% interval calculated above. The match is only roughly true, but it is good enough for most applied work.

If there is an `NA` value in the variable, any statistical function like `mean()` will return `NA`. You can fix this by using `na.rm = TRUE` with the statistical function.

<!-- DK: Might make a nice looking plot. -->

### `group_by()`

We can the use `mean()` with `summarize()` to calculate the average `Temp` value of the entire year.

```{r}
airquality %>% 
  summarize(mean = mean(Temp))
```

However, let's say instead of a single mean temperature for the whole year, you would like 5 mean temperatures, one for each of the 5 months in which data was gathered. In other words, we would like to compute the mean temperature split by month. We can do this by "grouping" temperature observations by the values of `Month`. Consider:


```{r}
airquality %>% 
  group_by(Month)
```

The data is the same as before, but note the "Groups" message at the top. R is informing you that this tibble has been grouped at that any operation you perform now will be done for each `Month`. Example: 

```{r}
airquality %>% 
  group_by(Month) %>% 
  summarize(mean = mean(Temp))
```

Notice the message R sends us. The warning means that the tibble which issues forth from the end of the pipe has been "ungrouped". This means the group attribute we applied with `group_by()` has been removed. This behavior is the (sensible) default.

The proper way to handle the situation, here and everywhere else that we use `group_by()` and `summarize()`, is to specify the `.groups` argument.

```{r}
airquality %>% 
  group_by(Month) %>% 
  summarize(mean = mean(Temp), 
            .groups = "drop")
```

This code does the same thing as the first version, but does not issue a warning, since we have made an affirmative decision to drop any grouping variables.

The `group_by()` function doesn't change data frames by itself. Rather it changes the *meta-data*, or data about the data, specifically the grouping structure. It is only after we apply the `summarize()` function that the tibble changes.

If you have a tibble which has been grouped, you can remove the grouping variable by using `ungroup()`. When your R code is behaving in a weird way, especially when it is "losing" rows, the problem is often solved by using `ungroup()` in the pipe.



## Advanced Plots

<!-- DK: Other stuff needed here? after_stat; plotting two geoms at once; scale_x for changing labels; scales:: for axis formatting. coord_cartesian() and xlim/ylim, which allows for zooming/rescaling graphs. Other? -->

<!-- - using scales to change the name and labels of a legend. -->

<!-- - facet_grid() to facet by more than 1 variable. -->

<!-- 1) Plots are ordered alphabetically if it is a character and by levels if it is a factor. Dates appear differently in plots. reorder() is useful. Cover this at start of advanced plotting. -->

<!-- 2) Purpose of Advanced plotting is to show the tools for making the plots that appear in socviz.co, chapters 3 and 4.  -->

<!-- BG: teach scalexcont, scaleycont, afterstat, late tricks in the book, do not worrry about building gapminder -->


In the previous section we have seen the three components that every plot must include: data, data mappings, and a geom. You may have found that the graphics from before look a bit boring. Luckily, the Grammar of Graphics allows us to add more layers on our own in order to customize our plots. We will go over some of the additional layers here, but the [Data Visualization with ggplot2 Cheat Sheet](https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf) is a great resource to refer to as your ggplot visualizations get more complicated.

### Faceting

Let's start by introducing a new concept called *faceting*.  Faceting is used when we'd like to split a particular visualization by the values of another variable. This will create multiple copies of the same type of plot with matching x and y axes, but whose content will differ. 


Before we proceed, let's create a subset of the data `gapminder` to use.

```{r}
gapminder_filt <- gapminder %>% 
      filter(year == 2007, continent != "Oceania")
```


Let's plot our filtered data using `geom_point()`

```{r}
ggplot(data = gapminder_filt, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp, 
                     color = continent)) +
  geom_point()
```

As you can see, it is quite difficult to compare the continents despite the colors. It would be much easier if we could "split" this scatterplot by the 5 continents in the dataset. In other words, we would create plots of `gdpPercap` and `lifeExp` for each `continent` separately. We do this by using the function `facet_wrap()` with the argument `~ continent`.

```{r}
ggplot(data = gapminder_filt, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp, 
                     color = continent)) +
  geom_point() +
  facet_wrap(~ continent)
```

This is way better! However, R chooses by default 2 plots per row, so that Asia and Europe are below the other two continents. We can specify the number of rows and columns in the grid by using the `nrow` argument inside of `facet_wrap()`. Let's get all continents in a row by setting `nrow` to 1.


```{r}
ggplot(data = gapminder_filt, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp, 
                     color = continent)) +
  geom_point() +
  facet_wrap(~ continent, 
             nrow = 1)
```

This is way better! However, R chooses by default 2 plots per row, so that Asia and Europe are below the other two continents. We can specify the number of rows and columns in the grid by using the `nrow` and `ncol` arguments inside of `facet_wrap()`. Let's get all continents in a row by setting `nrow` to 1.


Let's now add a trend line `geom_smooth()` to our faceted plot.

```{r}
ggplot(data = gapminder_filt, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp, 
                     color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  geom_smooth(method = "lm", 
              formula = y ~ x, 
              se = FALSE)
```

As expected, we can see a positive correlation between economic development and life expectancy on all continents. Now it is also clearer that Asia is on average at about the same level as the Americas, but there are more countries in Asia that are at both extremes.


### Stats

Consider the following histogram.

```{r}
ggplot(data = gapminder, 
       mapping = aes(x = lifeExp))+ 
  geom_histogram(bins = 20, 
                 color = "white")
```

Recall the y-aesthetic of a histogram, the count of the observations in each bin, gets computed for you automatically. We can use the  `after_stat()` argument within `geom_histogram()` to generate percent values as our y-aesthetic. 


```{r message = FALSE}
ggplot(data = gapminder, 
       mapping = aes(x = lifeExp))+ 
  geom_histogram(aes(y = after_stat(count/sum(count)), 
                   bins = 20))
```

 
### Coordinate Systems

Next, we can specify the type of coordinate system. In most cases we will use Cartesian coordinate systems. A coordinate system that is used most often is `coord_flip()`. This is actually just a Cartesian coordinate system, but as the name suggests, it simply swaps the axes.


Consider the faceted scatterplot we previously created. 

```{r}
ggplot(data = gapminder_filt, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp, 
                     color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  geom_smooth(method = "lm", 
              formula = y ~ x, 
              se = FALSE)
```

Let's now add the layer `coord_flip()` to flip the axises.

```{r}
ggplot(data = gapminder_filt, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp, 
                     color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  geom_smooth(method = "lm", 
              formula = y ~ x, 
              se = FALSE) +
  coord_flip()
```

As can be seen, `lifeExp` is now on the x-axis and `gdpPercap` on the y-axis. Compared to the previous plot it is now easier to observe the distribution of life expectancy in the respective continents. For example, we can see that many countries in Africa are at about 55 years, in the Americas and Asia at 75 years, and in Europe at 80 years. However, we think it makes more sense to consider `lifeExp` as the dependent variable, so don’t use `coord_flip()` in the subsequent plots.


### Axis Limits and Scales

We can also manipulate the limits of the axes by using `xlim()` and `ylim()`. For example, assume that we are only interested in countries with a GDP per capita from 0 to 30000. We can tell R as follows that we only want to see this range. Note that, because `data` is the first argument and `mapping` is the second to `ggplot()`, we don't actually have to name the arguments. We can just provide them, as long as they are in the correct order.

```{r warning = FALSE}
ggplot(gapminder_filt, 
       aes(x = gdpPercap, y = lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent) +
  xlim(0, 30000)
```

We can see that the GDP per capita on the x-axis is now only shown from 0 to 30000. 


We can also change the scaling of the axes. For example, it might be useful to display the axes with `scale_x_log10()` or `scale_y_log10()` on a logarithmic scale. Let's try this out on GDP per capita. Also, note that we can (lazily!) not provide the explicit `x` and `y` argument names to `aes()` as long as we provide the values in the right order: `x` comes before `y`. 

```{r}
ggplot(gapminder_filt, 
       aes(gdpPercap, lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent) +
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE) + 
  scale_x_log10()
```

<!-- BG: Need scale_x_cont and scale_y_cont here -->

### Text

Recall we use `labs()` to add labels and titles to our plots. 
We can also change labels inside the plots. Wouldn't it be great if we knew which country each point refers to? We can do this with the help of `geom_text()`:

```{r}
ggplot(gapminder_filt, 
       aes(gdpPercap, lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE) + 
  scale_x_log10() +
  labs(title = "Life Expectancy and GDP per Capita (2007)",
       subtitle = "Selected Nations by Continent",
       x = "GDP per Capita, USD",
       y = "Life Expectancy, Years",
       caption = "Source: Gapminder") +
  geom_text(aes(label = country), size = 2, 
            color = "black", check_overlap = TRUE)
```

Let's breakdown the code within `geom_text()`. We included a new aesthetic called *label*. This defines the character variable which will be used as the basis for the labels. We set `label` to country so each point corresponds to the country is represents. We set the text font by setting `size` to 2, and we set the text color using `color`. Finally, we included the argument `check_overlap = TRUE` to make sure the names of the countries were legible. 


### Themes

**Themes** can be used to change the overall appearance of a plot without much effort. We add themes as layers to our plots. You can find an overview of the different themes in ggplot [here](https://ggplot2.tidyverse.org/reference/ggtheme.html).


Let's consider the following faceted scatterplot.

```{r}

gapminder %>%
  filter(continent != "Oceania") %>%
  filter(year == max(year)) %>% 
  ggplot(aes(gdpPercap, lifeExp, color = continent)) +
    geom_point(alpha = 0.7) +
    geom_smooth(method = "lm", 
                formula = y ~ x,
                se = FALSE) + 
    facet_wrap(~continent, nrow = 2) +
    labs(title = "Life Expectancy and GDP per Capita",
         subtitle = "Connection between GDP and life expectancy is weakest in Africa",
         x = "GDP per Capita in USD",
         y = "Life Expectancy") +
    scale_x_log10(breaks = c(500, 5000, 50000)) 
```

Let's now add a theme to our faceted scatterplot. We will use the theme `theme_economist()` to make our plot look like the plots in the *The Economist*.

```{r}
gapminder %>%
  filter(continent != "Oceania") %>%
  filter(year == max(year)) %>% 
  ggplot(aes(gdpPercap, lifeExp, color = continent)) +
    geom_point(alpha = 0.7) +
    geom_smooth(method = "lm", 
                formula = y ~ x,
                se = FALSE) + 
    facet_wrap(~continent, nrow = 2) +
    labs(title = "Life Expectancy and GDP per Capita",
         subtitle = "Connection between GDP and life expectancy is weakest in Africa",
         x = "GDP per Capita in USD",
         y = "Life Expectancy") +
    scale_x_log10(breaks = c(500, 5000, 50000)) +
  theme_economist()
```

This looks pretty good. However, notice the legend the top of our graph.  It crowds our graph and takes away from the most important part: the data. We can use `theme()` to customize the non-data parts of our plots such as background, gridlines, and legends. Let's de-clutter the graph by removing our legend. We can do this by using the `legend.position` argument and setting it to "none".

<!-- DK: Make this new code consistent with the above. -->

```{r}
gapminder %>%
  filter(continent != "Oceania") %>%
  filter(year == max(year)) %>% 
  ggplot(aes(gdpPercap, lifeExp, color = continent)) +
    geom_point(alpha = 0.7) +
    geom_smooth(method = "lm", 
                formula = y ~ x,
                se = FALSE) + 
    facet_wrap(~continent, nrow = 2) +
    labs(title = "Life Expectancy and GDP per Capita",
         subtitle = "Connection between GDP and life expectancy is weakest in Africa",
         x = "GDP per Capita in USD",
         y = "Life Expectancy") +
    scale_x_log10(breaks = c(500, 5000, 50000),
                  labels = scales::dollar_format(accuracy = 1)) + 
    theme_economist() +
    theme(legend.position = "none")
  
```

Great. Now our graph is easier to visualize. In addition, note that the `theme()` function also offers a wide [selection](https://ggplot2.tidyverse.org/reference/theme.html) of functions for manually changing individual elements.


## Summary

In this chapter, we first looked at basic coding terminology and concepts that we deal with when programming with R. We then learned about the three basic components that make up each plot: data, mapping, and one or more geoms. The **ggplot2** package offers a wide range of geoms that we can use to create different types of plots. Next, we examined the "super package" **tidyverse**, which includes helpful tools for visualization. It also offers features for importing and manipulating data, which is the main topic of Chapter @\ref(wrangling). Lastly, we explored advanced plotting features such as axis scaling, faceting, and themes. 

The main *statistical* concept we discussed was that of a "distribution," a collection of related numbers. In R, these numbers are usually stored as a variable inside of a tibble. "Frequency distribution" and "empirical distribution" are common phrases to describe such objects. We learned how to calculate statistics --- often referred to as "summary statistics" --- about these distributions. The most important involve two concepts: the center of the distribution and the variability of the distribution. The center is most often summarized with the *mean* (`mean()`) or the *median* (`median()`). Variability is measured with the standard deviation (`sd()`) or the scaled median absolute deviation (`mad()`) or the 95% confidence interval (`quantile(probs = c(0.025, 0.975))`).   


Recall the plot we began the chapter with:


```{r, echo = FALSE}
gap_p
```

You now know enough to make plots like this by yourself.

It is important to know that we have only seen a small part of what R offers. For example, we can use the **gganimate** package to bring a slightly modified version of our gapminder plot to life:

```{r, cache = TRUE, out.width = "100%"}
library(gganimate)

gap_anim <- gapminder %>%
  filter(continent != "Oceania") %>%
  ggplot(aes(gdpPercap, lifeExp, color = continent)) +
    geom_point(show.legend = FALSE, alpha = 0.7) +
    facet_wrap(~continent, nrow = 1) +
    scale_size(range = c(2, 12)) +
    scale_x_log10() +
    labs(subtitle = "Life Expectancy and GDP per Capita (1952-2007)",
         x = "GDP per Capita, USD",
         y = "Life Expectancy, Years") +
    theme_linedraw() +
    transition_time(year) +
    labs(title = "Year: {frame_time}") +
    shadow_wake(wake_length = 0.1, alpha = FALSE)

gap_anim
```

The **plotly** package makes our plot interactive. Click on it and explore!


```{r, cache = TRUE, message = FALSE, warning = FALSE, out.width = "100%"}
library(plotly)

ggplotly(gap_p)
```

A beautiful plot is just a collection of steps, each simple enough on its own. We have taught you (some of) these steps. Time to start walking on your own.
