# Visualization

Everyone loves visualizations. 

<!-- DK: economist theme causes the subtitle (and other labels) to bleed into the title. Can we clean this up? Use a different theme? -->

<!-- DK: Would be nice if the plot featured a statistical measure of some kind (maybe a 95% confidence interval) so that we have a motivation for learning about distributions. Probably need a very different plot. Maybe something political? Election 2020? -->

<!-- DK: Plot should be much cooler, perhaps even using big data. And an associated map. Precinct level voting changes 2012 -> 2016 -> 2020, perhaps. -->


```{r, message = FALSE, echo = FALSE}
library(tidyverse)
library(ggthemes)
library(scales)
library(gapminder)

gap_p <- gapminder %>%
  filter(continent != "Oceania") %>%
  filter(year == max(year)) %>% 
  ggplot(aes(gdpPercap, lifeExp, color = continent)) +
    geom_point(alpha = 0.7) +
    geom_smooth(method = "lm", 
                formula = y ~ x,
                se = FALSE) + 
    facet_wrap(~continent, nrow = 2) +
    labs(title = "Life Expectancy and GDP per Capita",
         subtitle = "Connection between GDP and life expectancy is weakest in Africa",
         x = "GDP per Capita in USD",
         y = "Life Expectancy") +
    scale_x_log10(breaks = c(500, 5000, 50000),
                  labels = scales::dollar_format(accuracy = 1)) + 
    theme_economist() +
    theme(legend.position = "none")

gap_p
```

Once you have read this chapter, and completed the associated tutorials, you will be able to create graphics like this one with your data. Join us on the journey.


## Getting Started

This chapter focuses on **ggplot2**, one of the core packages in the **tidyverse**. To access the datasets, help pages, and functions that we will use in this chapter, load the tidyverse:

```{r} 
library(tidyverse)
```

That one line of code loads all the packages associated with the **tidyverse**, packages which you will use in almost every data analysis. It also tells you which functions from the tidyverse conflict with functions in base R or with other packages you might have loaded. (We hide these and other messages in this book because they are ugly.)

If you run this code and get the error message “there is no package called ‘tidyverse’”, you’ll need to install the package first using the code below. Then, run `library(tidyverse)` once again.


```{r, eval = FALSE}
install.packages("tidyverse")
```

If we need to be explicit about where a function (or dataset) comes from, we’ll use double colon operator (::) like this: `package::function()`. For example, `ggplot2::ggplot()` tells you explicitly that we are using the `ggplot()` function from the **ggplot2** package.

### How do I code in R?

Unlike other statistical software programs like Excel, SPSS, or Minitab that provide [point-and-click](https://en.wikipedia.org/wiki/Point_and_click) interfaces, R is an [interpreted language](https://en.wikipedia.org/wiki/Interpreted_language). This means you have to type in commands written in *R code*. In other words, you have to code/program in R. Note that we'll use the terms "coding" and "programming" interchangeably.


If you want a slower introduction than we are providing here, check out the short book, [*Getting Used to R, RStudio, and R Markdown*](https://rbasics.netlify.com/). @usedtor2016 includes screencast recordings that you can follow along and pause as you learn. They include an introduction to R Markdown, a tool used for reproducible research in R.


### Tips



* **Remember that computers are not actually that smart.** You have to tell a computer everything it needs to do. The instructions you give your computer can not have any mistakes in them.

* **Take the "copy, paste, and tweak" approach.** Take existing code that you know works and modify it to suit your need. With more experience, you move away from this approach and write code from scratch.

* **You need to practice every day.** Learning to code/program is like learning a foreign language. Daily practice is the *sine qua non* for excellence.


### Terminology

<!-- DK: Should have an entire tutorial devoted to this section, hitting every single section. -->

We use a different font to distinguish regular text from `computer_code`. 

* *Console pane*: where you enter in commands in RStudio.

* *Running code*: the act of telling R to put our code into action. 

* *Objects*: where values are stored in R. You can *assign* values to objects and display the contents of objects. Use the assignment operator `<-` to do so.  You can choose almost any name you like for an object, as long as the name does not begin with a number or a special character like `+`, `*`, `-`, `/`, `^`, etc.

For example we can save the result of `4 + 3` to an object named `x`. 

```{r}
x <- 4 + 3
```

Now, whenever we type `x`, the value of 4 + 3 will show:

```{r}
x
```


* *Data types*: integers, doubles/numerics, logicals, and characters. Integers are values like -1, 0, 2, 4092. Doubles or numerics are a larger set of values containing both the integers but also fractions and decimal values like -24.932 and 0.8. Logicals are either `TRUE` or `FALSE`. Characters are text like "cabbage" or "Hamilton", "The Wire is the greatest TV show ever". Characters are often denoted with the quotation marks around them.

* *Vectors*: a collection of values. These are created using the `c()` function, where `c` stands for "combine". 

For example, `c(2, 4, 6, 8)` creates a four element vector of numeric values.

```{r}
c(1, 2, 3, 4)
```


* *Factors*: used to represent "categorical data." We will go into detail about these and other variable types in Chapter \@ref(wrangling).

* *Data frames*: rectangular spreadsheets of data. Rows correspond to *units*. Columns correspond to *variables*. Modern data frames are called *tibbles*.  

* *Boolean algebra*: `TRUE/FALSE` statements and mathematical operators such as `<` (less than), `<=` (less than or equal to), and `!=` (not equal to). For example, `4 + 2 >= 3` will return `TRUE`, but `3 + 5 <= 1` will return `FALSE`. 

* *Inclusion*: Tested for with the `%in%` operator. For example, `"B" %in% c("A", "B")` returns `TRUE` while `"C" %in% c("A", "B")` returns `FALSE`. 

* *Equality*: Tested for using `==`. For example, `2 + 1 == 3` compares `2 + 1` to `3` and is legal R code, returning `TRUE`. On the other hand, `2 + 1 = 3` will return an error because we can not assign one number to another. 

* *Logical operators*: `&` representing "and" as well as `|` representing "or." For example, `(2 + 1 == 3) & (2 + 1 == 4)` returns `FALSE` since both clauses are not `TRUE` (only the first clause is `TRUE`). On the other hand, `(2 + 1 == 3) | (2 + 1 == 4)` returns `TRUE` since at least one of the two clauses is `TRUE`. 

* *Functions*: perform tasks, and are also called *commands*. They take in inputs called *arguments* and return outputs. You can either manually specify a function's arguments or use the function's *default values*. 

For example, `sqrt(64)` will return the square root of its argument 64.

```{r}
sqrt(64)
```

* *Help files*: provide documentation for various functions and datasets. You can bring up help files by adding a `?` before the name of a function or data frame and then run this in the console.

* *Code comments*: are text placed after a `#` symbol. Nothing will be run after a `#` symbol, which is useful when you include human readable comments in your code, as you always should.

* *Errors, warnings, and messages*: generally reported in a red font. When there is an error, the code will not run. Read (and/or google) the message and try fix it. Warnings don't prevent code from completing. For example, if you create a scatterplot based on data with two missing values, you will see this warning: `Warning: Removed 2 rows containing missing values (geom_point)`. Messages are similar. In both cases, you should fix the underlying issue until the warning/message goes away.    

### Examining `trains`

Most data comes to us in "spreadsheet"-type format.  These "spreadsheet"-type datasets are called *data frames* or *tibbles* in R. Let's explore the `trains` tibble in the **primer.data** package. This data comes from @enos2014, which investigated attitudes toward immigration among Boston commuuters.


```{r}
library(primer.data)
trains
```

Let's unpack this output:

* A `tibble` is a specific kind of data frame. This particular data frame has `r nrow(trains)` rows corresponding to different *units*, meaing people in this case. 

* The tibble also has `r ncol(trains)` columns corresponding to *variables* which describe each observation. 

* We see, by default, the top 10 rows. R is only showing the first 10 rows, since that is all that you probably want to see at first. You can see more (or fewer) rows with scrolling or using the `print()` command:

```{r}
print(trains, n = 15)
```

The `n` argument to `print()` tells R the number of rows you want to see. 

### Exploring data frames

There are many ways to get a feel for the data contained in a data frame such as `trains`. 


#### `view()`

Run `view(trains)` in the Console in RStudio, either by typing it or cutting-and-pasting it into the Console pane. Explore this data frame in the resulting pop up viewer. 

`view()` allows us to explore the different *variables* listed in the columns. Observe that there are many different types of variables.  Some of the variables are *quantitative*. These variables are numerical in nature.  Other variables here, including `gender` and  `treatment` are *categorical*.


#### `glimpse()`

We can also explore a data frame using the `glimpse()`function. 

```{r}
glimpse(trains)
```

Observe that `glimpse()` will give you the first few entries of each variable in a row after the variable name.  In addition, the *data type* of the variable is given immediately after each variable's name, inside `< >`. 

`dbl` refers to "double", which is computer coding terminology for quantitative/numerical variables. `int` refers to "integer" and is another data type that also represents quantitative/numerical variables.`fct` refers to "factor" and describes a variable that is nominal, meaning a member of a smallish number of categories. `chr` is for character data.


#### `$` operator

The `$` operator allows us to extract and then explore a single variable within a data frame.

```{r}
trains$age
```

We used the `$` operator to extract only the `age` variable and return it as a vector. 

<!-- DK: Other stuff? Or save $ for elsewhere? -->


## Basic Plots

There are three essential components to a plot:

* `data`: the dataset containing the variables of interest.
* `geom`: the geometric object to display, e.g., scatterplot, line, bar.
* `aes`: aesthetic attributes of the geometric object. The most important are the names of the variables should be on the x and y axes. Additional attributes include color and size. Aesthetic attributes are *mapped* to variables in the dataset.

Let's look at an example of a basic scatterplot using data from @enos2014 for `r nrow(trains)` Boston commuters.

```{r}
ggplot(data = trains, 
       mapping = aes(x = age, 
                     y = income)) + 
  geom_point()
```

Notice how the three components( i.e.`data`, `geom`, and `aes`) are specified in the `ggplot()` function.

Once we've specified these components, we then add *layers* to the plot using the `+` sign. The most essential layer to add to a plot is the layer that specifies which type of `geom`etric object we want the plot to involve: points, lines, bars, and others. In our graph above, the geom we used is `geom_point()`. This tells R we want our graph to include points (i.e. a scatterplot). 

If you look at the code above, you will notice the `+` sign comes at the end of the code line and not at the beginning. When adding layers to a plot, start a new line after the `+` so that the code for each layer is on a new line.

Let's now dive deeper into `geom_point()`.  

### `geom_point()`

*Scatterplots*, also called *bivariate plots*, allow you to visualize the *relationship* between two numerical variables. 

Recall our scatterplot from above.

```{r}
ggplot(data = trains, 
       mapping = aes(x = age, 
                     y = income)) + 
  geom_point()
```

Let's break down this code, piece-by-piece.

* The `data` argument is set to `trains` via `data = trains`.

* The `aes`thetic `mapping` is set via `mapping = aes(x = age, y = income)`. Here, we map `age` to the `x` axis and `income` maps to the `y` axis.

* The `geom`etric object is specified to `geom_point()`, telling R we want a scatterplot. We added a layer using the `+` sign.

If we do not specify the `geom`etric object, we have a blank plot like such:

```{r}
ggplot(data = trains, 
       mapping = aes(x = age, 
                     y = income))
```


In addition to mapping variables to the `x` and `y` axes, we can also map variables to `color`. 

```{r}
ggplot(data = trains, 
       mapping = aes(x = age, 
                     y = income,
                     color = party)) + 
  geom_point()
```

<!-- Size here? -->

We use the function `labs()` to add a plot title, axis labels, subtitles, and captions to our graph. By default, R simply uses the names of variables for axes and legends. Let's now add better titles and labels.

```{r}
ggplot(data = trains, 
       mapping = aes(x = age, 
                     y = income)) + 
  geom_point() +
  labs(title = "Age and Income Among Boston Commuters",
       subtitle = "Older commuters don't seem to make more money",
       x = "Age",
       y = "Income",
       caption = "Data source: Enos (2014)")
```

Note that like with `geom`s, we add a layer using `+` when creating `labs()`for our plot. In general, every plot should gave a title and axes labels. You should also add a `subtitle`, the purpose of which is to give a short "main point" of the graphic. What do you want the viewer to notice? You should also provide the source for the data, usually via the `caption` argument.

Let's now take a tour of some of the more useful geoms.

### `geom_jitter()`

Consider a different scatter plot using the `trains` data.

```{r}
ggplot(data = trains, 
       mapping = aes(x = att_start, 
                     y = att_end)) + 
  geom_point() +
  labs(title = "Immigration Attitudes Among Boston Commuters",
       subtitle = "Attitudes did not change much after the experiment",
       x = "Attitude Before Experiment",
       y = "Attitude After Experiment",
       caption = "Data source: Enos (2014)")
```

The problem with this display is "overplotting." Because attitudes are measured as integers, we don not know if a given point represents just one person or a dozen. There are two methods we can use to address overplotting: transparency and jitter.

**Method 1: Changing the transparency**

We can change the transparency/opacity of the points by using the `alpha` argument within `geom_point()`. The `alpha` argument can be set to any value between `0` and `1`, where `0` sets the points to be 100% transparent and `1` sets the points to be 100% opaque. By default, `alpha` is set to `1`. In other words, if we don't explicitly set an `alpha` value, R will use `alpha = 1`.

Let's now add an `alpha` argument to our scatterplot. 

```{r}
ggplot(data = trains, 
       mapping = aes(x = att_start, 
                     y = att_end)) + 
  geom_point(alpha = 0.2) +
  labs(title = "Immigration Attitudes Among Boston Commuters",
       subtitle = "Attitudes did not change much after the experiment",
       x = "Attitude Before Experiment",
       y = "Attitude After Experiment",
       caption = "Data source: Enos (2014)")
```

Note that there is no `aes()` surrounding `alpha = 0.2`. This is because we are not mapping a variable to an aesthetic attribute, but rather only changing the default setting of `alpha`. In fact, you'll receive an error if you try to change the second line to read `geom_point(aes(alpha = 0.2))`.

**Method 2: Jittering the points**

We can also decide to jitter the points on the plot. We do this by replacing `geom_point()` with `geom_jitter()`. Keep in mind that jittering is strictly a visualization tool; even after creating a jittered scatterplot, the original values saved in the data frame remain unchanged. 

In order to specify how much jitter to add, we use the `width` and `height` arguments to `geom_jitter()`. This corresponds to how hard you'd like to shake the plot in horizontal x-axis units and vertical y-axis units, respectively. It is important to add just enough jitter to break any overlap in points, but not to the extent where you alter the original pattern in points.

```{r}
ggplot(data = trains, 
       mapping = aes(x = att_start, 
                     y = att_end)) + 
  geom_jitter() +
  labs(title = "Immigration Attitudes Among Boston Commuters",
       subtitle = "Attitudes did not change much after the experiment",
       x = "Attitude Before Experiment",
       y = "Attitude After Experiment",
       caption = "Data source: Enos (2014)")
```

When deciding whether to jitter a scatterplot or use the `alpha` argument to `geom_point()`, know that there is no single right answer. We suggest you play around with both methods to see which one better emphasizes the point you are trying to make. 

### `geom_line()`

Linegraphs show the relationship between two numerical variables when the variable on the x-axis, also called the *explanatory* variable, is of a sequential nature. In other words, there is an inherent ordering to the variable. 

```{r, echo = FALSE}
knitr::include_graphics("https://imgs.xkcd.com/comics/decline.png")
```

The most common examples of linegraphs have some notion of time on the x-axis: hours, days, weeks, years, etc. Since time is sequential, we connect consecutive observations of the variable on the y-axis with a line. Linegraphs that have some notion of time on the x-axis are also called *time series* plots. 

Let's plot the median duration of unemployment in the United States over the last 50 years.

<!-- BG: need data set to show graph here.  -->

```{r}
ggplot(data = economics,
       mapping = aes(x = date, y = uempmed)) +
  geom_line() +
  labs(title = "Unemployment Duration in the United States: 1965 -- 2015",
       subtitle = "Dramatic increase in duration after the Great Recesssion",
       x = "Date",
       y = "Median Duration in Weeks")
```

Almost every aspect of the code used to create this plot is identical to our scatter plots, except for the geom we used.


### `geom_histogram()`

A histogram is a plot that visualizes the *distribution* of a numerical value as follows:

1. We first cut up the x-axis into a series of *bins*, where each bin represents a range of values. 
1. For each bin, we count the number of observations that fall in the range corresponding to that bin.
1. Then for each bin, we draw a bar whose height marks the corresponding count.

Let's consider the `income` variable of the the `trains` data frame. Pay attention to how we have changed the two arguments to `ggplot()`. We have removed `data = ` and `mapping = `. The code still works because R functions allow for passing in arguments by *position*. The first argument to `ggplot()` is the data. So, we don't need to tell R that `trains` is the value for `data`. R assumes that it is because we passed it in first. Similarly, the second argument to `ggplot()` is `mapping`, so R assumes that `aes(x = income)` is the value we want for mapping because it is the second item passed in.

```{r}
ggplot(trains, 
       aes(x = income)) +
  geom_histogram()
```

Note the message printed above:

> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

You would get the same message if you ran this code yourself. Try it!

The message is telling us that the histogram was constructed using `bins = 30` for 30 equally spaced bins. This is the default value; unless you override this default number of bins with a number you specify, R will choose 30 by default. Because this is an important aspect of making a histogram, R insists on informing you with this message. You make this message go away by specifying the `bin` number yourself, as you should always do.

Let's specify `bins` and also add some labels.

```{r}
ggplot(trains, 
       aes(x = income)) +
  geom_histogram(bins = 50) +
  labs(title = "Income Among Boston Commuter",
       subtitle = "Why are there so few people with `middle' incomes?",
       x = "Income",
       y = "Count",
       caption = "Data source: Enos (2014)")
```


Unlike scatterplots and linegraphs, there is now only one variable being mapped in `aes()`. Here, that variable is `income`. The y-aesthetic of a histogram, the count of the observations in each bin, gets computed for you automatically. Furthermore, the geometric object layer is now a `geom_histogram()`. 


We can use the `fill` argument to change the color of the actual bins. Let's set `fill` to "steelblue". 

```{r}
ggplot(trains, 
       aes(x = income)) +
  geom_histogram(bins = 50,
                 fill = "steelblue") +
  labs(title = "Income Among Boston Commuter",
       subtitle = "Why are there so few people with `middle' incomes?",
       x = "Income",
       y = "Count",
       caption = "Data source: Enos (2014)")
```

We can also adjust the number of bins in our histogram in one of two ways:

1. By adjusting the number of bins via the `bins` argument to `geom_histogram()`. 

2. By adjusting the width of the bins via the `binwidth` argument to `geom_histogram()`. 

In this data, however, there are not many unique values for `income`, so neither approach will have much effect. Replace `income` with `age` if you want to experiment with these options.


### `geom_bar()` 

Similarly to histograms, `geom_bar()` visualizes the distribution of a categorical variable. This is a simpler task, as we are simply counting different categories within a categorical variable, also known as the *levels* of the categorical variable. Often the best way to visualize these different counts, also known as *frequencies*, is with barplots.


```{r}
ggplot(data = trains, 
       mapping = aes(x = race)) +
  geom_bar()
```


`geom_col()` is very similar to `geom_bar()`, except that `geom_col()` requires you to calculate the number of observations in each category ahead of time. `geom_bar()` does the calculation for you.


#### No pie charts!

One of the most common plots used to visualize the distribution of categorical data is the pie chart. While they may seem harmless enough, pie charts actually present a problem in that humans are unable to judge angles well. @robbins2013 argues that we overestimate angles greater than 90 degrees and we underestimate angles less than 90 degrees. In other words, it is difficult for us to determine the relative size of one piece of the pie compared to another.  

While pie charts present information in a way such that comparisons must be made by comparing angles, barplots are more effective because they present the information in a way such that comparisons between categories can be made with single horizontal lines.


#### Two categorical variables

Barplots are a very common way to visualize the frequency of different categories, or levels, of a single categorical variable. Another use of barplots is to visualize the *joint* distribution of two categorical variables at the same time. Let's look at `race`, as well as `region`, in the `nes` data by using the `fill` argument inside the `aes()` aesthetic mapping. Recall the `fill` aesthetic corresponds to the color used to fill the bars.

```{r}
ggplot(trains, 
       aes(x = race, fill = treatment)) +
  geom_bar()
```

This is an example of a *stacked barplot*.  While simple to make, in certain aspects it is not ideal. For example, it is difficult to compare the heights of the different colors between the bars, corresponding to comparing the number of people of different races within each region. 

An alternative to stacked barplots are *side-by-side barplots*, also known as *dodged barplots*. The code to create a side-by-side barplot includes a `position = "dodge"` argument added inside `geom_bar()`. In other words, we are overriding the default barplot type, which is a *stacked* barplot, and specifying it to be a side-by-side barplot instead.


```{r}
ggplot(trains, 
       aes(x = race, fill = treatment)) +
  geom_bar(position = "dodge")
```

Whites are over-reprsented in the Control group even though the `treatment` was assigned at random.

### `geom_smooth()`

We can add trend lines to the plots we create using the `geom_smooth()` function. 

<!-- DK: Will handle geom_smooth().explain method, se, and formula -->

Recall the following scatterplot from our previous work. 

```{r}
ggplot(trains, 
       aes(x = att_start, 
           y = att_end)) + 
  geom_point() +
  labs(title = "Immigration Attitudes Among Boston Commuters",
       subtitle = "Attitudes did not change much after the experiment",
       x = "Attitude Before Experiment",
       y = "Attitude After Experiment",
       caption = "Data source: Enos (2014)")
```

We can add a trend line to our graph by adding the layer `geom_smooth()`. Including trend lines allow us to visualize the relationship between `att_start` and `att_end`.

```{r}
ggplot(trains, 
       aes(x = att_start, 
           y = att_end)) + 
  geom_point() +
  labs(title = "Immigration Attitudes Among Boston Commuters",
       subtitle = "Attitudes did not change much after the experiment",
       x = "Attitude Before Experiment",
       y = "Attitude After Experiment",
       caption = "Data source: Enos (2014)") +
  geom_smooth()
```

Do you see the message R gives us? R is telling us that we need to specify the `method` and `formula` argument, just the way it told us to provide `bins` argument when we used `geom_histogram()` before.

Let's add the argument `method = "lm"`, where `lm` stands for linear model. This causes the fitted line to be straight rather than curved. Let's also add the argument `formula = y ~ x`. This makes both messages go away. Again, R was not giving us an error before. It was simply telling us what options it was using since we did not specify the options ourselves.

*Always include enough detail in your code to make those messages disappear.*

```{r}
ggplot(trains, 
       aes(x = att_start, 
           y = att_end)) + 
  geom_point() +
  labs(title = "Immigration Attitudes Among Boston Commuters",
       subtitle = "Attitudes did not change much after the experiment",
       x = "Attitude Before Experiment",
       y = "Attitude After Experiment",
       caption = "Data source: Enos (2014)") +
  geom_smooth(method = "lm", 
              formula = y ~ x)
```

Notice the gray section surrounding both sides of both of the lines we plotted. This area is called the confidence interval, which is set to a 95% confidence interval by default. We will learn about confidence intervals in Chapter \@ref(probability). You can make the shaded disappear by adding `se = FALSE` as another argument to `geom_smooth()`.

### `geom_density()`

Recall the histogram we plotted in the `geom_histogram()` section.

```{r}
ggplot(trains, 
       aes(x = income)) +
  geom_histogram(bins = 50) +
  labs(title = "Income Among Boston Commuter",
       subtitle = "Why are there so few people with `middle' incomes?",
       x = "Income",
       y = "Count",
       caption = "Data source: Enos (2014)")
```

We can change `geom_histogram()` to `geom_density()` to make a density plot, which is a smoothed version of the histogram. 

```{r}
ggplot(trains, 
       aes(x = income)) +
  geom_density() +
  labs(title = "Income Among Boston Commuter",
       subtitle = "Why are there so few people with `middle' incomes?",
       x = "Income",
       y = NULL,
       caption = "Data source: Enos (2014)")
```

<!-- DK: Other stuff to finish this section with? -->


## The Tidyverse

Going forward, most `ggplot()` code will omit the `data = ` and `mapping = ` explicit naming of arguments while relying on the default ordering of those arguments. Most of the time, we include argument names and, as a rule, you should to. But we create so many plots in *The Primer* that these ommissions are unlikely to cause problems.


### Data wrangling 

We can't use all the beautiful plots that we learned in the previous chapter until we have "wrangled" the data into a convenient shape. In this chapter, we'll introduce a series of functions from the **tidyverse** collection of packages which help with wrangling, and everything else we need to do to work with data. Such functions include:

* `filter()` to pick out the *rows* we want to keep from a tibble .

* `select()` to pick out the *columns* we want to keep from a tibble.

* `arrange()` the rows in a tibble, in either ascending or descending order. 

*`mutate()` to create new columns.

* `group_by()` the rows of a tibble. This function assigns different rows to be part of the same *group*. This allows statistics to be calculated for each group *separately*. You will usually use `group_by` with `summarize()`.

*`summarize()` the data. It creates a new data frame comprised of summary statistics for one (or more) rows for each grouped variable, or for the tibble as a whole if it is ungrouped. 

### The pipe operator: `%>%`

Before we start data wrangling, let's first introduce a nifty tool that gets loaded with the **dplyr** package included in the tidyverse: the pipe operator `%>%`. The pipe operator allows us to combine multiple operations in R into a single sequential *chain* of actions.

Recall the (modified) code for one of our graphics above.

```{r}
ggplot(trains, 
       aes(x = att_start, 
           y = att_end)) + 
  geom_point() +
  geom_smooth(method = "lm", 
              formula = y ~ x) +
  labs(title = "Immigration Attitudes Among Boston Commuters",
       subtitle = "Attitudes did not change much after the experiment",
       x = "Attitude Before Experiment",
       y = "Attitude After Experiment",
       caption = "Data source: Enos (2014)")

```

Much like how the `+` sign has to come at the end of lines when constructing plots --- because we are building the plot layer-by-layer --- the pipe operator `%>%` has to come at the end of lines because we are building a data wrangling pipeline step-by-step.


The code below filters the `trains` dataset to only row for men.

```{r}
trains %>% 
  filter(gender == "Male")
```

A single *chain* of data wrangling operations is formed here by combining verb-named functions into a single sequence using the pipe operator `%>%`. 

### `filter()` rows

```{r, echo = FALSE, fig.cap = "Diagram of filter() rows operation."}
knitr::include_graphics("01-visualization/images/filter.png")
```

The `filter()` function here works much like the "Filter" option in Microsoft Excel; it allows you to specify criteria about the values of a variable in your dataset and then filters out only the rows that match that criteria.

Let's revisit the code from the previous section.

```{r}
trains %>% 
  filter(gender == "Male")
```

The result of using `filter()` will be a tibble with just the rows that you want. When we alter our data, it can be a good idea to save the result in a new data frame by using the `<-` assignment operator.

```{r}
trains_men <- trains %>% 
  filter(gender == "Male")
```

Let's break down the code here: 

Here we assigned our new data to an object named `trains_men` via `trains_men <-`. Because we assigned this modified data frame to `trains_men`, it is a separate entity from the initial `trains` data frame. If, however, we had written the code as `trains <- trains` we would have overwritten the already-existing tibble. 

We take the `trains` tibble and  *then* `filter()` so that only those observations where the `gender` equals "Male" are included. We test for equality using the double equal sign `==` and not a single equal sign `=`. In other words, `filter(gender = "Male")` will produce an error. This is a convention across many programming languages. If you are new to coding, you'll probably forget to use the double equal sign `==` a few times before you get the hang of it.

You can use other operators beyond just the `==` operator.

- `>` corresponds to "greater than"
- `<` corresponds to "less than"
- `>=` corresponds to "greater than or equal to"
- `<=` corresponds to "less than or equal to"
- `!=` corresponds to "not equal to." The `!` is used in many programming languages to indicate "not."

Furthermore, you can combine multiple criteria using operators that make comparisons:

- `|` corresponds to "or"
- `&` corresponds to "and"

For example, let's `filter()` the `trains` tibble to include only women who are Republicans and younger than 40.

```{r}
trains %>% 
  filter(gender == "Female" & 
           party == "Republican" &
           age < 40)
```

`filter()` should often be among the first verbs you consider applying to your data. This cleans your dataset to only those rows you care about.

<!-- DK: More examples? -->

### `select` variables

```{r, echo = FALSE, fig.cap = "Diagram of select() columns."}
knitr::include_graphics("01-visualization/images/select.png")
```

Using the `filter()` function we were able to pick out specific *rows* from the dataset. The `select()` function allows us to pick specific *columns* (variables) instead.

Use `glimpse()` to see the names of the variables in `trains`:

```{r}
glimpse(trains)
```

However, if you only need two of these variables, say `gender` and `treatment`. You can `select()` just these two:

```{r}
trains %>% 
  select(gender, treatment)
```

Let's say instead you want to drop, or de-select, certain variables. Do that with the minus (`-`) sign:

```{r}
trains %>% 
  select(-gender, -liberal, -party, -age)
```

Another way of selecting columns/variables is by specifying a range of columns using the `:` operator. 

```{r}
trains %>% 
  select(gender:age)
```

This will `select()` all columns between the two specified variables. 
The `select()` function can also be used to rearrange columns when used with the `everything()` helper function. We can out the `treatment` and `gender` variables first with:  

```{r}
trains %>% 
  select(treatment, gender, everything())
```

The helper functions `starts_with()`, `ends_with()`, and `contains()` can be used to select variables/columns that match those conditions. Examples:

```{r}
trains %>% 
  select(starts_with("a"))
```

### `slice()` and `pull()` and `[]`

`slice()` and `pull()` are additional functions that you can use to pick out specific rows or columns within a data frame.

Using `slice()` gives us specific rows from the `gapminder` tibble:

```{r}
trains %>% 
  slice(2:5)
```

Unlike `filter()`, `slice()` relies on numeric order of the data. 

`pull()` grabs out a variable as a vector, rather than leaving it within a tibble, as `select()` does:

```{r}
trains %>% 
  slice(2:5) %>% 
  pull(age)
```


### `arrange()` rows

One of the most commonly performed data wrangling tasks is to sort a data frame's rows in the alphanumeric order of one of the variables. Unlike `filter()` or `select()`, `arrange()` does not remove any rows or columns from the data frame. Instead,  `arrange()` allows us to sort/reorder a tibble's rows according to the values of a specific variable.

Let's start by selecting some relevant variables:

```{r}
trains %>% 
  select(treatment, gender, age)
```

Sort by `age`

```{r}
trains %>% 
  select(treatment, gender, age) %>% 
  arrange(age)
```

Notice how the `age` column is sorted from the smallest values to the largest. This is because `arrange()` always returns rows sorted in ascending order by default. To switch the ordering to be in descending order instead, we use the `desc()` function like so:

```{r}
trains %>% 
  select(treatment, gender, age) %>% 
  arrange(desc(age))
```

This is the first of many "pipes" which we will create in *The Primer*. First, we have the `trains` tibble. Second, we pipe that to the `select()` function. Third, we pipe the results of `select()` to the `arrange()` function.


### `mutate()` 

```{r, echo = FALSE, fig.cap = "Diagram of mutate() columns."}
knitr::include_graphics("01-visualization/images/mutate.png")
```

Another common transformation of data is to create new variables based on existing ones. In other words, we want to make a new column using already existing column We can use the `mutate()` function from the **dplyr** package, which takes existing variables and mutates them to create new ones. 


Recall that the `income` variable in the `trains` tibble is in dollars. Let's use `mutate()` to create a new variable where income is in thousands of dollars. (We use `select()` at the start of the pipe so that it is easier to see the new and old variables at the same time.)

```{r}
trains %>% 
  select(gender, income) %>% 
  mutate(income_in_thousands = income / 1000)
```


Notice that we have a newly created column at the right-hand side of our tibble named `income_in_thousands`. 

When creating new variables we can also *overwrite* the original data frames like `trains` like so:

```{r}
trains <- trains %>% 
  mutate(income_in_thousands = (income) / 1000)
```

Why overwrite a data frame `trains` instead of assigning the result to a new data frame like `trains_new`? As a rough rule of thumb, as long as you are not losing original information that you might need later, it is acceptable practice to overwrite existing data frames with updated ones, as we did here. On the other hand, why did we not overwrite the variable `income`, but instead created a new variable called `income_in_thousands`?  Because if we did this, we would have erased the original information contained in `income` and would not be able to use them in the future. 


#### `ifelse()`

<!-- Discuss case_when()? -->

<!-- DK: Edit these lines from R4DS. Are there other functions which we should mention here? Do something sensible with group_by. To get `_` to work in caption title. Found at https://github.com/rstudio/bookdown/issues/209  -->

`ifelse()` is often used within calls to `mutate()`. It has three arguments. The first argument `test` should be a logical vector. The result will contain the value of the second argument, `yes`, when test is TRUE, and the value of the third argument, `no`, when it is FALSE. 

Imagine that we want to create a new variable `old`, which is TRUE when `age > 50` and FALSE otherwise. 

```{r}
trains %>% 
  select(age) %>% 
  mutate(old = ifelse(age > 50, TRUE, FALSE))
```

Another function similar to `ifelse()`, is `dplyr::case_when()`. `case_when()` is particularly useful inside mutate when you want to create a new variable that relies on a complex combination of existing variables. Note that there is a different version of `ifelse()` in **dplyr**: `if_else()`. This works exactly the same as the standard version but is somewhat more robust. 

<!-- DK: More details? -->


### `summarize()`

A musical interlude inspired by the Tidyverse:

```{r, echo = FALSE}
knitr::include_app("https://www.youtube.com/watch?v=p8Py9C8iq2s")
```

The next common task when working with data frames is to compute *summary statistics*. Summary statistics are single numerical values that summarize a large number of values. Commonly known examples of summary statistics include the *mean* (also called the average) and the *median* (the middle value). Other examples of summary statistics that might not immediately come to mind include the *sum*, the *minimum*, the *maximum*, and the *standard deviation*.

The function `summarize()` allows us to calculate these statistics on individual columns of the dataset. Example:

```{r}
trains %>% 
  summarize(mean = mean(age), 
            std_dev = sd(age))
```

The `mean()` and `sd()` summary functions go inside the `summarize()`  function. (You can also use the British English spelling:  `summarise()`.) The `summarize()` function takes in a data frame and returns a data frame with only one row corresponding to the summary statistics. 


### Statistics for distributions

A variable in a tibble is a column, a vector of values. We sometimes refer to this vector as a "distribution." This is somewhat sloppy in that a distribution can be many things, most commonly a mathematical formula. But, strictly speaking, a "frequency distribution" or an "empirical distribution" is a list of values, so this usage is not unreasonable.

*There are two distinct concepts: a distribution and a set values drawn from that distribution.*  But, in everyday use, we use "distribution" to both. When given a distribution (meaning a vector of numbers), we often use `geom_histogram()` or `geom_density()` to graph it. But, sometimes, we don't want to look at the whole thing. We just want some summary measures which report the key aspects of the distribution. The two most important attributes of a distribution are its *center* and its *variation* around that center.

We use `summarize()` to calculate statistics for a variable, a column, a vector of values or a distribution. Note the language sloppiness. For the purposes of this book, "variable," "column," "vector," and "distribution" all mean the same thing. Popular statistical functions include: `mean()`, `median()`, `min()`, `max()`, `n()` and `sum()`. Functions which may be new to you include three measures of the "spread" of a distribution: `sd()` (the standard deviation), `mad()` (the scaled median absolute deviation) and `quantile()` (use to calculate an *interval* which includes a specified proportion of the values). 

**`mean()`**

The mean, or average, is the most commonly reported measure of the center of a distribution.  The mean is the sum of all of the data elements divided by how many elements there are. If we have $N$ data points, the mean is given by: 

$$\bar{x} = \frac{x_1 + x_2 + \cdots + x_N}{N}$$

**`median()`**

<!-- DK: Use math formula, like this one: https://math.stackexchange.com/questions/1097546/what-is-the-general-formula-for-calculating-the-median -->

The median is another commonly reported measure of the center of a distribution, calculated by first sorting the vector of values from smallest to largest. The middle element in the sorted list is the *median*.  If the middle falls between two values, then the median is the mean of those two middle values. The median and the mean are the two most common measures of the *center* of a distribution. The median is more stable, less affected by outliers. There is no widely accepted symbol for the median, although $\tilde{x}$ is not uncommon.

**`sd()`**

The standard deviation* (*sd*) of a distribution is a measure of its variation around the mean.  

$$\text{sd} = \sqrt{\frac{(x_1 - \bar{x})^2 + (x_2 - \bar{x})^2 + \cdots + (x_n - \bar{x})^2}{n - 1}}$$
<!-- DK: More detail. How much does it cover? Plot a not-too-weirdish distribution and show two sd's on it. -->


**`mad()`**

The scaled median absolute deviation (*mad*) is a measure of variation around the median.  It is not as popular as the standard deviation. The formula for calculating *mad* is a bit mysterious.


$$\text{mad} = 1.4826 \times \text{median}(abs(x - \tilde{x}))$$

The basic idea for both *sd* and *mad* is that we need a measure of variation around the center of the distribution. *sd* uses the mean, $\bar{x}$, as its estimate of the center while *mad* uses the median, $\tilde{x}$. Because *mad* uses the absolute difference, as opposed to the squared difference, it is more robust to outliers. The 1.4826 multiplier causes the *mad* and the *sd* to be identical in the (important) case of standard normal distributions, a topic we will cover in Chapter \@ref(wrangling).

<!-- DK: Could have more conversation. -->

<!-- DK: These formulas are inconsistent, with one using vectors. Should make these consistent, perhaps even give formula for median. -->


**`quantile()`**

The *quantile* of a distribution is the value of that distribution which occupies a specific percentile location in the sorted list of values. 

The 5th percentile distribution is the point below which 5% of the data falls. The 95th percentile is, similarly, the point below which 95% of the data falls. The 50th percentile, the median, splits the data into two separate, and equal, parts. The minimum is at the 0th percentile. The maximum is at the 100th percentile.

Let's take a look at the `poverty` variable in the `kenya` tibble from the **primer.data** package. `poverty` is the percentage of residents in each community with incomes below the poverty line. Let's first confirm that connections between `quantile()` and simpler functions.

```{r}
c(min(kenya$poverty), median(kenya$poverty), max(kenya$poverty))
quantile(kenya$poverty, probs = c(0, 0.5, 1))
```

The `probs` argument allows us to specify the percentile(s) we want. Two of the most important percentiles are the 2.5th and 97.5th because they define the *95% interval*, a central range which includes 95% of the values.

```{r}
quantile(kenya$poverty, probs = c(0.025, 0.975))
```

The interval between these two percentiles includes 95% of all the values in the distribution. Depending on the context, this interval is sometimes called a "confidence interval" or "uncertainty interval" or "compatibility interval." Different percentile ranges create intervals of different widths. 

An interesting fact is that, most of the time, the 95% confidence interval is, roughly, the same as the center of the distribution $\pm$ two times the standard deviation.

In distributions without major outliers, the mean/median and sd/mad will be very similar to each other.

```{r}
kenya %>% 
  summarise(mean = mean(poverty),
            median = median(poverty),
            sd = sd(poverty),
            mad = mad(poverty))
```

It follows that intervals defined with them will be similar.

```{r}
kenya %>% 
  summarise(interval_1_bottom = mean(poverty) - 2 * sd(poverty),
            interval_1_top = mean(poverty) + 2 * sd(poverty),
            interval_2_bottom = median(poverty) - 2 * mad(poverty),
            interval_2_top = median(poverty) + 2 * mad(poverty))
```

These intervals are similar to each other and similar to the 95% interval calculated above. The match is only roughly true, but it is good enough for most applied work.

If there is an `NA` value in the variable, any statistical function like `mean()` will return `NA`. You can fix this by using `na.rm = TRUE` with the statistical function.

<!-- DK: Might make a nice looking plot. -->

### `group_by()`

We can the use `mean()` with `summarize()` to calculate the average age for all the people in `trains`, as we did above.

```{r}
trains %>% 
  summarize(mean = mean(age))
```

What if we want the mean `age` for each `gender`? Consider:


```{r}
trains %>% 
  group_by(gender)
```

The data is the same as before, but note the "Groups" message at the top. R is informing you that this tibble has been grouped so that any operation you perform now will be done for each `gender`. 

```{r}
trains %>% 
  group_by(gender) %>% 
  summarize(mean = mean(age))
```

Notice the message R sends us. The warning means that the tibble which issues forth from the end of the pipe has been "ungrouped". This means the group attribute we applied with `group_by()` has been removed. This behavior is the (sensible) default.

The proper way to handle the situation, here and everywhere else that we use `group_by()` and `summarize()`, is to specify the `.groups` argument.

```{r}
trains %>% 
  group_by(gender) %>% 
  summarize(mean = mean(age),
            .groups = "drop")
```

This code does the same thing as the first version, but does not issue a message, since we have made an affirmative decision to drop any grouping variables.

The `group_by()` function doesn't change data frames by itself. Rather it changes the *meta-data*, or data about the data, specifically the grouping structure. It is only after we apply the `summarize()` function that the tibble changes.

If you have a tibble which has been grouped, you can remove the grouping variable by using `ungroup()`. 

*When your R code is behaving in a weird way, especially when it is "losing" rows, the problem is often solved by using `ungroup()` in the pipeline.*



## Advanced Plots

<!-- DK: Other stuff needed here? after_stat; plotting two geoms at once; scale_x for changing labels; scales:: for axis formatting. coord_cartesian() and xlim/ylim, which allows for zooming/rescaling graphs. Other? -->

<!-- - using scales to change the name and labels of a legend. -->

<!-- - facet_grid() to facet by more than 1 variable. -->

<!-- 1) Plots are ordered alphabetically if it is a character and by levels if it is a factor. Dates appear differently in plots. reorder() is useful. Cover this at start of advanced plotting. -->

<!-- 2) Purpose of Advanced plotting is to show the tools for making the plots that appear in socviz.co, chapters 3 and 4.  -->

<!-- BG: teach scalexcont, scaleycont, afterstat, late tricks in the book, do not worrry about building gapminder -->


In the previous section we have seen the three components that every plot must include: data, data mappings, and a geom. You may have found that the graphics from before look a bit boring. Luckily, the Grammar of Graphics allows us to add more layers on our own in order to customize our plots. We will go over some of the additional layers here, but the [Data Visualization with ggplot2 Cheat Sheet](https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf) is a great resource to refer to as your ggplot visualizations get more complicated.

### Faceting

Let's start by introducing a new concept called *faceting*.  Faceting is used when we'd like to split a particular visualization by the values of another variable. This will create multiple copies of the same type of plot with matching x and y axes, but whose content will differ. 


Before we proceed, let's create a subset of the data `gapminder` to use.

```{r}
gapminder_filt <- gapminder %>% 
      filter(year == 2007, continent != "Oceania")
```


Let's plot our filtered data using `geom_point()`

```{r}
ggplot(data = gapminder_filt, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp, 
                     color = continent)) +
  geom_point()
```

As you can see, it is quite difficult to compare the continents despite the colors. It would be much easier if we could "split" this scatterplot by the 5 continents in the dataset. In other words, we would create plots of `gdpPercap` and `lifeExp` for each `continent` separately. We do this by using the function `facet_wrap()` with the argument `~ continent`.

```{r}
ggplot(data = gapminder_filt, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp, 
                     color = continent)) +
  geom_point() +
  facet_wrap(~ continent)
```

This is way better! However, R chooses by default 2 plots per row, so that Asia and Europe are below the other two continents. We can specify the number of rows and columns in the grid by using the `nrow` argument inside of `facet_wrap()`. Let's get all continents in a row by setting `nrow` to 1.


```{r}
ggplot(data = gapminder_filt, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp, 
                     color = continent)) +
  geom_point() +
  facet_wrap(~ continent, 
             nrow = 1)
```

This is way better! However, R chooses by default 2 plots per row, so that Asia and Europe are below the other two continents. We can specify the number of rows and columns in the grid by using the `nrow` and `ncol` arguments inside of `facet_wrap()`. Let's get all continents in a row by setting `nrow` to 1.


Let's now add a trend line `geom_smooth()` to our faceted plot.

```{r}
ggplot(data = gapminder_filt, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp, 
                     color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  geom_smooth(method = "lm", 
              formula = y ~ x, 
              se = FALSE)
```

As expected, we can see a positive correlation between economic development and life expectancy on all continents. Now it is also clearer that Asia is on average at about the same level as the Americas, but there are more countries in Asia that are at both extremes.


### Stats

Consider the following histogram.

```{r}
ggplot(data = gapminder, 
       mapping = aes(x = lifeExp))+ 
  geom_histogram(bins = 20, 
                 color = "white")
```

Recall the y-aesthetic of a histogram, the count of the observations in each bin, gets computed for you automatically. We can use the  `after_stat()` argument within `geom_histogram()` to generate percent values as our y-aesthetic. 


```{r message = FALSE}
ggplot(data = gapminder, 
       mapping = aes(x = lifeExp))+ 
  geom_histogram(aes(y = after_stat(count/sum(count)), 
                   bins = 20))
```

 
### Coordinate Systems

Next, we can specify the type of coordinate system. In most cases we will use Cartesian coordinate systems. A coordinate system that is used most often is `coord_flip()`. This is actually just a Cartesian coordinate system, but as the name suggests, it simply swaps the axes.


Consider the faceted scatterplot we previously created. 

```{r}
ggplot(data = gapminder_filt, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp, 
                     color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  geom_smooth(method = "lm", 
              formula = y ~ x, 
              se = FALSE)
```

Let's now add the layer `coord_flip()` to flip the axises.

```{r}
ggplot(data = gapminder_filt, 
       mapping = aes(x = gdpPercap, 
                     y = lifeExp, 
                     color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  geom_smooth(method = "lm", 
              formula = y ~ x, 
              se = FALSE) +
  coord_flip()
```

As can be seen, `lifeExp` is now on the x-axis and `gdpPercap` on the y-axis. Compared to the previous plot it is now easier to observe the distribution of life expectancy in the respective continents. For example, we can see that many countries in Africa are at about 55 years, in the Americas and Asia at 75 years, and in Europe at 80 years. However, we think it makes more sense to consider `lifeExp` as the dependent variable, so don’t use `coord_flip()` in the subsequent plots.


### Axis Limits and Scales

We can also manipulate the limits of the axes by using `xlim()` and `ylim()`. For example, assume that we are only interested in countries with a GDP per capita from 0 to 30000. We can tell R as follows that we only want to see this range. Note that, because `data` is the first argument and `mapping` is the second to `ggplot()`, we don't actually have to name the arguments. We can just provide them, as long as they are in the correct order.

```{r warning = FALSE}
ggplot(gapminder_filt, 
       aes(x = gdpPercap, y = lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent) +
  xlim(0, 30000)
```

We can see that the GDP per capita on the x-axis is now only shown from 0 to 30000. 


We can also change the scaling of the axes. For example, it might be useful to display the axes with `scale_x_log10()` or `scale_y_log10()` on a logarithmic scale. Let's try this out on GDP per capita. Also, note that we can (lazily!) not provide the explicit `x` and `y` argument names to `aes()` as long as we provide the values in the right order: `x` comes before `y`. 

```{r}
ggplot(gapminder_filt, 
       aes(gdpPercap, lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent) +
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE) + 
  scale_x_log10()
```

<!-- BG: Need scale_x_cont and scale_y_cont here -->

### Text

Recall we use `labs()` to add labels and titles to our plots. 
We can also change labels inside the plots. Wouldn't it be great if we knew which country each point refers to? We can do this with the help of `geom_text()`:

```{r}
ggplot(gapminder_filt, 
       aes(gdpPercap, lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE) + 
  scale_x_log10() +
  labs(title = "Life Expectancy and GDP per Capita (2007)",
       subtitle = "Selected Nations by Continent",
       x = "GDP per Capita, USD",
       y = "Life Expectancy, Years",
       caption = "Source: Gapminder") +
  geom_text(aes(label = country), size = 2, 
            color = "black", check_overlap = TRUE)
```

Let's breakdown the code within `geom_text()`. We included a new aesthetic called *label*. This defines the character variable which will be used as the basis for the labels. We set `label` to country so each point corresponds to the country is represents. We set the text font by setting `size` to 2, and we set the text color using `color`. Finally, we included the argument `check_overlap = TRUE` to make sure the names of the countries were legible. 


### Themes

**Themes** can be used to change the overall appearance of a plot without much effort. We add themes as layers to our plots. You can find an overview of the different themes in ggplot [here](https://ggplot2.tidyverse.org/reference/ggtheme.html).


Let's consider the following faceted scatterplot.

```{r}

gapminder %>%
  filter(continent != "Oceania") %>%
  filter(year == max(year)) %>% 
  ggplot(aes(gdpPercap, lifeExp, color = continent)) +
    geom_point(alpha = 0.7) +
    geom_smooth(method = "lm", 
                formula = y ~ x,
                se = FALSE) + 
    facet_wrap(~continent, nrow = 2) +
    labs(title = "Life Expectancy and GDP per Capita",
         subtitle = "Connection between GDP and life expectancy is weakest in Africa",
         x = "GDP per Capita in USD",
         y = "Life Expectancy") +
    scale_x_log10(breaks = c(500, 5000, 50000)) 
```

Let's now add a theme to our faceted scatterplot. We will use the theme `theme_economist()` to make our plot look like the plots in the *The Economist*.

```{r}
gapminder %>%
  filter(continent != "Oceania") %>%
  filter(year == max(year)) %>% 
  ggplot(aes(gdpPercap, lifeExp, color = continent)) +
    geom_point(alpha = 0.7) +
    geom_smooth(method = "lm", 
                formula = y ~ x,
                se = FALSE) + 
    facet_wrap(~continent, nrow = 2) +
    labs(title = "Life Expectancy and GDP per Capita",
         subtitle = "Connection between GDP and life expectancy is weakest in Africa",
         x = "GDP per Capita in USD",
         y = "Life Expectancy") +
    scale_x_log10(breaks = c(500, 5000, 50000)) +
  theme_economist()
```

This looks pretty good. However, notice the legend the top of our graph.  It crowds our graph and takes away from the most important part: the data. We can use `theme()` to customize the non-data parts of our plots such as background, gridlines, and legends. Let's de-clutter the graph by removing our legend. We can do this by using the `legend.position` argument and setting it to "none".

<!-- DK: Make this new code consistent with the above. -->

```{r}
gapminder %>%
  filter(continent != "Oceania") %>%
  filter(year == max(year)) %>% 
  ggplot(aes(gdpPercap, lifeExp, color = continent)) +
    geom_point(alpha = 0.7) +
    geom_smooth(method = "lm", 
                formula = y ~ x,
                se = FALSE) + 
    facet_wrap(~continent, nrow = 2) +
    labs(title = "Life Expectancy and GDP per Capita",
         subtitle = "Connection between GDP and life expectancy is weakest in Africa",
         x = "GDP per Capita in USD",
         y = "Life Expectancy") +
    scale_x_log10(breaks = c(500, 5000, 50000),
                  labels = scales::dollar_format(accuracy = 1)) + 
    theme_economist() +
    theme(legend.position = "none")
  
```

Great. Now our graph is easier to visualize. In addition, note that the `theme()` function also offers a wide [selection](https://ggplot2.tidyverse.org/reference/theme.html) of functions for manually changing individual elements.


## Summary

In this chapter, we first looked at basic coding terminology and concepts that we deal with when programming with R. We then learned about the three basic components that make up each plot: data, mapping, and one or more geoms. The **ggplot2** package offers a wide range of geoms that we can use to create different types of plots. Next, we examined the "super package" **tidyverse**, which includes helpful tools for visualization. It also offers features for importing and manipulating data, which is the main topic of Chapter @\ref(wrangling). Lastly, we explored advanced plotting features such as axis scaling, faceting, and themes. 

The main *statistical* concept we discussed was that of a "distribution," a collection of related numbers. In R, these numbers are usually stored as a variable inside of a tibble. "Frequency distribution" and "empirical distribution" are common phrases to describe such objects. We learned how to calculate statistics --- often referred to as "summary statistics" --- about these distributions. The most important involve two concepts: the center of the distribution and the variability of the distribution. The center is most often summarized with the *mean* (`mean()`) or the *median* (`median()`). Variability is measured with the standard deviation (`sd()`) or the scaled median absolute deviation (`mad()`) or the 95% confidence interval (`quantile(probs = c(0.025, 0.975))`).   


Recall the plot we began the chapter with:


```{r, echo = FALSE}
gap_p
```

You now know enough to make plots like this by yourself.

It is important to know that we have only seen a small part of what R offers. For example, we can use the **gganimate** package to bring a slightly modified version of our gapminder plot to life:

```{r, cache = TRUE}
library(gganimate)

gap_anim <- gapminder %>%
  filter(continent != "Oceania") %>%
  ggplot(aes(gdpPercap, lifeExp, color = continent)) +
    geom_point(show.legend = FALSE, alpha = 0.7) +
    facet_wrap(~continent, nrow = 1) +
    scale_size(range = c(2, 12)) +
    scale_x_log10() +
    labs(subtitle = "Life Expectancy and GDP per Capita (1952-2007)",
         x = "GDP per Capita, USD",
         y = "Life Expectancy, Years") +
    theme_linedraw() +
    transition_time(year) +
    labs(title = "Year: {frame_time}") +
    shadow_wake(wake_length = 0.1, alpha = FALSE)

gap_anim
```

The **plotly** package makes our plot interactive. Click on it and explore!


```{r, cache = TRUE, message = FALSE, warning = FALSE}
library(plotly)

ggplotly(gap_p)
```

A beautiful plot is just a collection of steps, each simple enough on its own. We have taught you (some of) these steps. Time to start walking on your own.
