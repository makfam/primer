# Wrangling {#wrangling}


<!-- Replace flights with fec16? airlines are boring. Git rid of 538? alex was here.  -->

<!-- Add in other join diagrams, like the one used for for inner join. -->

<!-- Start joins with left_join, since it is most popular. -->

*Data science is data cleaning.*

Start by loading the packages which we will need in this chapter.

```{r, message = FALSE}
library(tidyverse)
library(primer.data)
library(lubridate)
library(janitor)
library(skimr)
library(nycflights13)
library(gapminder)
library(fivethirtyeight)
```

The **tidyverse** package will be used in every chapter. **primer.data** is the data package created specifically for this *Primer*. **lubridate** is a package for working with dates and times. **janitor** offers functions for cleaning up dirty data. **skimr** contains functions that are useful for providing summary statistics. **nycflights** includes data associated with flights out of New York City's three major airports. **gapminder** has annual data for countries going back more than 50 years. **fivethirtyeight** cleans up data from the [FiveThirtyEight](https://fivethirtyeight.com/) team.

## Data Gathering

The `read_csv()` function turns flat files into data frames. More specifically, it turns files whose values are separated by commas. Letâ€™s import a **c**omma **s**eparated **v**alues (hence "csv") file from the internet. The `read_csv()` function included in the **readr** package, which is one of the packages in the Tidyverse, is different than the `read.csv()` function that comes installed with R. Always use Tidyverse functions, if possible. 

Each comma from the csv file corresponds to a column, and the column names are taken from the first line of the file. Then, the function "guesses" an appropriate data type for each of the columns it creates. Sometimes csv files are a lot dirtier and require significant wrangling before you can explore the data and create usable graphics. 

The link below is for a file containing faculty gender data across departments at Harvard University. Note, the `file` argument of the `read_csv()` function is very flexible. Below, it takes a url. `file` can also be the absolute or relative paths for a file saved locally on your computer. 

```{r}
url <- "https://raw.githubusercontent.com/PPBDS/primer/master/02-wrangling/data/faculty.csv"

fac <- read_csv(file = url, 
                skip = 1)
fac
```

We need to "skip" the first row of the file because there is a code comment at the top, which has nothing to do with the actual data.

Run the `summary()` function on the `fac` tibble.

```{r}
summary(fac)
```

Don't confuse `summary()`, which is a built-in --- generally termed "Base" --- R function for providing a summary of an object, with `summarize()`, which is one of the most important functions from **dplyr**.

You may notice that something appears to be wrong with this dataset. Which department could have employed 644 male professors? We will explore this mystery later in the chapter.


### HTML

<!-- DK: Rewrite these sections. -->

The data we need to answer a question is not always in a handy spreadsheet. For example, we can find interesting data about murders in the US in [this Wikipedia page](https://en.wikipedia.org/w/index.php?title=Gun_violence_in_the_United_States_by_state): 

```{r}
url <- paste0("https://en.wikipedia.org/w/index.php?title=",
              "Gun_violence_in_the_United_States_by_state",
              "&direction=prev&oldid=810166167")
```

You can see the data table when you visit the webpage:

```{r, echo = FALSE}
knitr::include_graphics("02-wrangling/images/murders-data-wiki-page.png")
```

To get this data, we need to do some web scraping. 

Web scraping, or web harvesting, is the term we use to describe the process of extracting data from a website. The reason we can do this is because the information used by a browser to render webpages is received as a text file from a server. The text is code written in hyper text markup language (HTML). Every browser has a way to show the html source code for a page, each one different. On Chrome, you can use Control-U on a PC and command+option+U on a Mac. You will see something like this:

```{r, echo = FALSE}
knitr::include_graphics("02-wrangling/images/html-code.png")
```

Because this code is accessible, we can download the HTML file, import it into R, and then write programs to extract the information we need from the page. However, once we look at HTML code, this might seem like a daunting task. But we will show you some convenient tools to facilitate the process. To get an idea of how it works, here are a few lines of code from the Wikipedia page that provides the US murders data:

```
<table class="wikitable sortable">
<tr>
<th>State</th>
<th><a href="/wiki/List_of_U.S._states_and_territories_by_population" 
title="List of U.S. states and territories by population">Population</a><br />
<small>(total inhabitants)</small><br />
<small>(2015)</small> <sup id="cite_ref-1" class="reference">
<a href="#cite_note-1">[1]</a></sup></th>
<th>Murders and Nonnegligent
<p>Manslaughter<br />
<small>(total deaths)</small><br />
<small>(2015)</small> <sup id="cite_ref-2" class="reference">
<a href="#cite_note-2">[2]</a></sup></p>
</th>
<th>Murder and Nonnegligent
<p>Manslaughter Rate<br />
<small>(per 100,000 inhabitants)</small><br />
<small>(2015)</small></p>
</th>
</tr>
<tr>
<td><a href="/wiki/Alabama" title="Alabama">Alabama</a></td>
<td>4,853,875</td>
<td>348</td>
<td>7.2</td>
</tr>
<tr>
<td><a href="/wiki/Alaska" title="Alaska">Alaska</a></td>
<td>737,709</td>
<td>59</td>
<td>8.0</td>
</tr>
<tr>
```

You can actually see the data, except data values are surrounded by html code such as `<td>`. We can also see a pattern of how it is stored. If you know HTML, you can write programs that leverage knowledge of these patterns to extract what we want. We also take advantage of a language widely used to make webpages look "pretty" called Cascading Style Sheets (CSS).

Although we provide tools that make it possible to scrape data without knowing HTML, as a data scientist it is quite useful to learn some HTML and CSS. Not only does this improve your scraping skills, but it might come in handy if you are creating a webpage to showcase your work. 

### The rvest package

The tidyverse contains a web harvesting package called rvest. The first step using this package is to import the webpage into R. The package makes this quite simple:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
h <- read_html(url)
```


Note that the entire Murders in the US Wikipedia webpage is now contained in `h`. The class of this object is:

```{r}
class(h)
```

The rvest package is actually more general; it handles XML documents. XML is a general markup language (that's what the ML stands for) that can be used to represent any kind of data. HTML is a specific type of XML specifically developed for representing webpages. Here we focus on HTML documents.

Now, how do we extract the table from the object `h`? If we print `h`, we don't really see much:

```{r}
h
```

We can see all the code that defines the downloaded webpage using the `html_text` function like this:

```{r, eval=FALSE}
html_text(h)
```

We don't show the output here because it includes thousands of characters, but if we look at it, we can see the data we are after is stored in an HTML table: you can see this in this line of the HTML code above `<table class="wikitable sortable">`. The different parts of an HTML document, often defined with a message in between  `<` and `>`  are referred to as nodes. The rvest package includes functions to extract nodes of an HTML document: `html_nodes` extracts all nodes of different types and `html_node` extracts the first one. To extract the tables from the html code we use:

```{r} 
tab <- h %>% html_nodes("table")
```

Now, instead of the entire webpage, we just have the html code for the tables in the page:

```{r}
tab
```

The table we are interested is the first one:

```{r}
tab[[1]]
```

This is clearly not a tidy dataset, not even a data frame. In the code above, you can definitely see a pattern and writing code to extract just the data is very doable. In fact, rvest includes a function just for converting HTML tables into data frames:


```{r}
tab <- tab[[1]] %>% html_table
class(tab)
```

We are now much closer to having a usable data table:

```{r}
tab <- tab %>% setNames(c("state", "population", 
                          "total", "murder_rate")) 

head(tab)
```

We still have some wrangling to do. For example, we need to remove the commas and turn characters into numbers. Before continuing with this, we will learn a more general approach to extracting information from web sites.


### CSS selectors

The default look of a webpage made with the most basic HTML is quite unattractive. The aesthetically pleasing pages we see today are made using CSS to define the look and style of webpages. The fact that all pages for a company have the same style usually results from their use of the same CSS file to define the style. The general way these CSS files work is by defining how each of the elements of a webpage will look. The title, headings, itemized lists, tables, and links, for example, each receive their own style including font, color, size, and distance from the margin. CSS does this by leveraging patterns used to define these elements, referred to as selectors. An example of such a pattern, which we used above, is `table`, but there are many, many more. 

If we want to grab data from a webpage and we happen to know a selector that is unique to the part of the page containing this data, we can use the `html_nodes` function. However, knowing which selector can be quite complicated. 
In fact, the complexity of webpages has been increasing as they become more sophisticated. For some of the more advanced ones, it seems almost impossible to find the nodes that define a particular piece of data. However, selector gadgets actually make this possible.

SelectorGadget^[http://selectorgadget.com/] is piece of software that allows you to interactively determine what CSS selector you need to extract specific components from the webpage. If you plan on scraping data other than tables from html pages, we highly recommend you install it. A Chrome extension is available which permits you to turn on the gadget and then, as you click through the page, it highlights parts and shows you the selector you need to extract these parts. There are various demos of how to do this including rvest author Hadley Wickham's
vignette^[https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html] and other tutorials based on the vignette^[https://stat4701.github.io/edav/2015/04/02/rvest_tutorial/] ^[https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/].

### JSON

Sharing data on the internet has become more and more common. Unfortunately, providers use different formats, which makes it harder for data scientists to wrangle data into R. Yet there are some standards that are also becoming more common. Currently, a format that is widely being adopted is the JavaScript Object Notation or JSON. Because this format is very general, it is nothing like a spreadsheet. This JSON file looks more like the code you use to define a list. Here is an example of information stored in a JSON format:

```{r, echo = FALSE}
library(jsonlite)
example <- data.frame(name= c("Miguel", "Sofia", "Aya", "Cheng"), student_id = 1:4, exam_1 = c(85, 94, 87, 90), exam_2 = c(86, 93, 88, 91))
json <- toJSON(example, pretty = TRUE) 
json
```

The file above actually represents a data frame. To read it, we can use the function `fromJSON` from the **jsonlite** package. Note that JSON files are often made available via the internet. Several organizations provide a JSON API or a web service that you can connect directly to and from which you can obtain data. 


## Tibbles

Tibbles are a kind of data frame, useful for storing data in which we have the same number of observations for each variable. We can use the `tibble()` function to create data frames. Tibbles are composed of columns, each of which is a variable, and rows, each of which are the values for those variables for a single "unit" or "observation." Each column (i.e., each variable) can be of a different type: character, numeric, factor, date and so on.


### `tibble()`

```{r}
tibble(a = 2.1, b = "Hello", c = TRUE, d = 9L)
```

In our code, we specify the variable names (or the names of columns) as , `a`, `b`, and `c`. Under each variable, we give a different numerical value. Under each variable name, the data type is specified for the data within that column. A tibble can consist of one or more atomic vectors, the most important types of which are double, character, logical, and integer. The tibble above includes a variable of each type. (The "L" in "9L" tells R that we want `d` to be an integer rather than the default, which would be a double.)  When you print out a tibble, the variable type is shown below the variable name.


Variables should not begin with a number (like `54abc`) or include spaces (like `my var`). If you insist on using varible names like that, you must include backticks around each name when you reference them.

```{r}
tibble(`54abc` = 1, `my var` = 2, c = 3)
```

If we did not include the backticks, R would give us an error.

```{r, error = TRUE, renv.ignore = TRUE}
tibble(54abc = 1, my var = 2, c = 3)
```

It is sometimes easier to use the function `tribble()` to create tibbles.

```{r}
tribble(
  ~ var1, ~ `var 2`, ~ myvar,
  1,           3,      5,
  4,           6,      8,
)
```

The tildes specifies which rows have the column names. The formatting makes it easier, relative to specifying raw vectors, to see which values are from the same observation.


### `clean_names()`

Recall the `fac` tibble we created earlier in the Chapter.

```{r}
fac
```

Let's look at the department with 644 make professors.

```{r, error = TRUE, renv.ignore = TRUE}
fac %>%
  filter(Professors M == 644)
```

Notice that there is a space in the column name `Professors M`. Recall that we can solve this problem by using backticks.

```{r, error = TRUE}
fac %>%
  filter(`Professors M` == 644)
```

There is not a department called "total." And backticks are annoying. So we run the `clean_names()` function, from the **janitor** package.  

```{r, message = FALSE}
library(janitor)
fac %>%
  clean_names()
```

`clean_names()` fixes all our variable names. With a wide tibble (i.e., one with lots of variables), this is much easier than either renaming by hand or using backticks all the time.


It seems that one row of the table takes the sum of each of the other rows. This is likely the last row of the dataframe. Use `tail()` to print the last rows of the dataframe to check the final row.

```{r}
tail(fac)
```

We remove the "Total" row because it will affect any dplyr functions we try to run later. Check the `tail()` of the dataframe again to make sure that the proper row was removed.

```{r}
fac %>%
  filter(Concentration != "Total") %>%
  tail()
```

<!-- DK: Above could be more elegant. Why are we fixing messed up data in this section? -->

### `skim()`

The `skim()` function from the **skimr** package is a useful summary of a tibble  

```{r}
skim(fac)
```

Take a quick look at the section containing numeric variables. This is of particular interest because the right-most column offers insight into the potential distribution of the data. 


## Characters 

```{r, echo = FALSE, fig.cap = "Real data is nasty."}
knitr::include_graphics("02-wrangling/images/nasty.gif")
```

So far, our tibbles have been clean and wholesome, like the `gapminder` and `trains`. **Real life datasets will be much nastier.** You will bring data into R from the outside world and discover there are problems. We will now discuss common remedial tasks for cleaning and transforming character data, also known as strings. A *string* is one or more characters that are enclosed inside a pair of matching 'single' or "double quotes".


This lesson deals with things you can do with vectors of class `character`. `fruit`, `words`, and `sentences` are Tidyverse character vectors which we will use. Although we can manipulate character vectors directly, it is much more common, in real world situations, to work with vectors which are in a tibble.

We will use the `fruit` data, a vector with the names of different fruits, from the **stringr** package, which is automatically loaded when we issue `library(tidyverse)`.

```{r}
tbl_fruit <- tibble(fruit = fruit)
```


### Studying a single character vector

```{r}
tbl_fruit %>% 
  sample_n(size = 8)
```

Note that `sample_n()` selects a random set of rows from the tibble.

`str_detect()` determines if a character vector matches a pattern.  It returns a logical vector that is the same length as the input. Recall logicals are either `TRUE` or `FALSE`.

Which fruits actually use the word "fruit"?

```{r}
tbl_fruit %>% 
  mutate(fruit_in_name = str_detect(fruit, pattern = "fruit")) %>% 
  filter(fruit_in_name)

```


`str_length()` counts characters in your strings. Note this is different from the `length()` of the character vector itself.

```{r}
tbl_fruit %>% 
  mutate(name_length = str_length(fruit)) 
```

`str_sub()` extracts parts of a string.`The function takes start and end arguments which are vectorised.

```{r}
tbl_fruit %>% 
  mutate(first_three_letters = str_sub(fruit, 1, 3)) 
```

`str_c()` combines a character vector of length to a single string. This is similar to the normal `c()` function for creating a vector.

```{r}
tbl_fruit %>% 
  mutate(name_with_s = str_c(fruit, "s")) 
```

`str_replace()` replaces a pattern within a string.

```{r}
tbl_fruit %>% 
  mutate(capital_A = str_replace(fruit, 
                                 pattern = "a", 
                                 replacement = "A")) 
```


### Regular expressions with stringr

Sometimes, your string tasks cannot be expressed in terms of a fixed string, but can be described in terms of a pattern. Regular expressions, also know as "regexes," are the standard way to specify these patterns. In regexes, specific characters and constructs take on special meaning in order to match multiple strings.

To explore regular expressions, we will use the `str_detect()` function, which reports TRUE for any string which matches the pattern, and then `filter()` to see all the matches. For example, here are all the fruits which include a "w" in their name.

```{r}
tbl_fruit %>%
  filter(str_detect(fruit, pattern = "w"))
```


In the code below, the first metacharacter is the period `.` , which stands for any single character, except a newline (which by the way, is represented by `\n`). The regex `b.r` will match all fruits that have an "b", followed by any single character, followed by "r". Regexes are case sensitive.

```{r}
tbl_fruit %>%
  filter(str_detect(fruit, pattern = "b.r"))
```

Anchors can be included to express where the expression must occur within the string. The `^` indicates the beginning of string and `$` indicates the end. 

```{r}
tbl_fruit %>%
  filter(str_detect(fruit, pattern = "^w"))
```

```{r}
tbl_fruit %>%
  filter(str_detect(fruit, pattern = "o$"))
```

<!-- DK: Add back quantifiers? -->

<!-- DK: Show some raw string examples or cut this out. -->

### Raw strings

There are certain characters with special meaning in regexes, including `$ * + . ? [ ] ^ { } | ( ) \`. This makes things difficult when we want to use these characters within the strings instead of as regexes. Patters which want to search for these character, rather than use them as metacharacters, should make use of `r"()"`, the raw string constant in R. Consider:

<!-- DK: This is not very clear. -->

```{r}
r"(Do you use "airquotes" much?)"
```

The `\` preceding each quote is what is known as escaping. It signals to the computer that the quotations are part of the string. Now, what happens if we have a backslash within the string. 

```{r echo = FALSE}
knitr::include_graphics("https://imgs.xkcd.com/comics/backslashes.png")
```

```{r}
r"(\)"
```

As you can see, a backslash is used to escape the initial backslash.  

## Factors

Factors are categorical variables that may only take on a specified set of values. Thus, they are known as categorical variables, as they can be separated into categories. To manipulate factors we will use the **forcats** package, a core package in the tidyverse. 

It is easy to make factors with either `factor()`, `as.factor()` or `parse_factor()`.

```{r}
tibble(X = letters[1:3]) %>% 
  mutate(fac_1 = factor(X)) %>% 
  mutate(fac_2 = as.factor(X)) %>% 
  mutate(fac_3 = parse_factor(X))
```

Which of those three options is best depends on the situation. `factor()` is useful when you are creating a factor from nothing. `as.factor()` is best for simple transformations, especially of character variables, as in this example. `parse_factor()` is the most modern and powerful of the three. 

Let's use `gapminder$continent` as an example.

```{r}
str(gapminder$continent)
levels(gapminder$continent)
nlevels(gapminder$continent)
class(gapminder$continent)
```

To get a frequency table as a tibble, from a tibble, use `count()`. To get a similar result from a free-range factor, use `fct_count()`.

```{r}
gapminder %>% 
  count(continent)
fct_count(gapminder$continent)

```

### Dropping unused levels

Just because you drop all the rows corresponding to a specific factor level, the levels of the factor itself do not change. Sometimes all these unused levels can come back to haunt you later, e.g., in figure legends.

Watch what happens to the levels of `country` when we filter `gapminder` to a handful of countries.

```{r}
nlevels(gapminder$country)
h_gap <- gapminder %>%
  filter(country %in% c("Egypt", "Haiti", 
                        "Romania", "Thailand", 
                        "Venezuela"))
nlevels(h_gap$country)
```

Even though `h_gap` only has data for a handful of countries, we are still schlepping around all levels from the original `gapminder` tibble.

How can you get rid of them? The base function `droplevels()` operates on all the factors in a data frame or on a single factor. The function `fct_drop()` operates on a factor.

```{r}
h_gap_dropped <- h_gap %>% 
  droplevels()
nlevels(h_gap_dropped$country)

# Use fct_drop() on a free-range factor

h_gap$country %>%
  fct_drop() %>%
  levels()
```

### Change order of the levels

By default, factor levels are ordered alphabetically. 

```{r}
gapminder$continent %>%
  levels()
```

We can also order factors by:

1. Frequency: Make the most common level the first and so on.
1. Another variable: Order factor levels according to a summary statistic for another variable.

Let's order continent by frequency using `fct_infreq()`.

```{r}
gapminder$continent %>% 
  fct_infreq() %>%
  levels()
```

We can also have the frequency print out backwards using `fct_rev()`.

```{r}
gapminder$continent %>% 
  fct_infreq() %>%
  fct_rev() %>% 
  levels()
```

These two barcharts of frequency by continent differ only in the order of the continents. Which do you prefer? We show the code for just the second one.

```{r, echo = FALSE}
gapminder %>% 
  ggplot(aes(x = continent)) +
    geom_bar() +
    coord_flip()
```


```{r}
gapminder %>% 
  mutate(continent = fct_infreq(continent)) %>% 
  mutate(continent = fct_rev(continent)) %>% 
  ggplot(aes(x = continent)) +
    geom_bar() +
    coord_flip()
```


Let's now order `country` by another variable, forwards and backwards. This other variable is usually quantitative and you will order the factor according to a grouped summary. The factor is the grouping variable and the default summarizing function is `median()` but you can specify something else.

```{r}
# Order countries by median life expectancy

fct_reorder(gapminder$country, 
            gapminder$lifeExp) %>% 
  levels() %>% 
  head()

# Order according to minimum life exp instead of median

fct_reorder(gapminder$country, 
            gapminder$lifeExp, min) %>% 
  levels() %>% 
  head()

# Backwards!

fct_reorder(gapminder$country, 
            gapminder$lifeExp, 
            .desc = TRUE) %>% 
  levels() %>% 
  head()
```

Why do we reorder factor levels? It often makes plots much better! When a factor is mapped to x or y, it should almost always be reordered by the quantitative variable you are mapping to the other one. 

Compare the interpretability of these two plots of life expectancy in North and South American countries in 2007. The only difference is the order of the `country` factor. Which one do you find easier to learn from?

```{r}
gapminder %>% 
  filter(year == 2007, 
         continent == "Americas") %>% 
  ggplot(aes(x = lifeExp, y = country)) + 
    geom_point()
```


```{r}
gapminder %>% 
  filter(year == 2007, 
         continent == "Americas") %>% 
  ggplot(aes(x = lifeExp, 
             y = fct_reorder(country, lifeExp))) + 
    geom_point()
```

<!-- Use `fct_reorder2()` when you have a line chart of a quantitative x against another quantitative y and your factor provides the color. This way the legend appears in the same order as the data! Note, the order is taken by the right side of the plot (not the left). Contrast the legend on the left with the one on the right.  -->

<!-- ```{r legends-made-for-humans, fig.show = 'hold', out.width = '49%', out.height="80%", echo = FALSE} -->
<!-- h_countries <- c("Egypt", "Haiti", "Romania", "Thailand", "Venezuela") -->
<!-- h_gap <- gapminder %>% -->
<!--   filter(country %in% h_countries) %>%  -->
<!--   droplevels() -->
<!-- ggplot(h_gap, aes(x = year, y = lifeExp, color = country)) + -->
<!--   geom_line() -->
<!-- ggplot(h_gap, aes(x = year, y = lifeExp, -->
<!--                   color = fct_reorder2(country, year, lifeExp))) + -->
<!--   geom_line() + -->
<!--   labs(color = "country") -->
<!-- ``` -->

<!-- Sometimes you just want to hoist one or more levels to the front. Why? Because we said so. This resembles what we do when we move variables to the front with `dplyr::select(special_var, everything())`. -->

<!-- ```{r} -->
<!-- h_gap$country %>%  -->
<!--   levels() -->

<!-- h_gap$country %>%  -->
<!--   fct_relevel("Romania", "Haiti") %>%  -->
<!--   levels() -->
<!-- ``` -->

<!-- This might be useful if you are preparing a report for, say, the Romanian government. The reason for always putting Romania first has nothing to do with the data, it is important for external reasons and you need a way to express this. -->

### Recode the levels

Sometimes you have better ideas about what certain levels should be. This is called recoding.

```{r}
i_gap <- gapminder %>% 
  filter(country %in% c("United States", "Sweden", 
                        "Australia")) %>% 
  droplevels()

i_gap$country %>% 
  levels()

i_gap$country %>%
  fct_recode("USA" = "United States", "Oz" = "Australia") %>% 
  levels()
```

### Grow a factor

Let's create two tibbles, each with data from two countries and dropped unused factor levels.

```{r}
df1 <- gapminder %>%
  filter(country %in% c("United States", "Mexico"), year > 2000) %>%
  droplevels()
df2 <- gapminder %>%
  filter(country %in% c("France", "Germany"), year > 2000) %>%
  droplevels()
```

The `country` factors in `df1` and `df2` have different levels.

```{r}
levels(df1$country)
levels(df2$country)
```

Can you just combine them?

```{r}
c(df1$country, df2$country)
```

Umm, no. That is wrong on many levels! Use `fct_c()` to do this.

```{r}
fct_c(df1$country, df2$country)
```

## Lists

```{r, echo = FALSE, fig.cap = "Subsetting a list, visually."}
knitr::include_graphics("02-wrangling/images/lists-subsetting.png")
```

Earlier, we briefly introduced lists. Lists are a type of vector that is a step up in complexity from atomic vectors, because lists can contain other lists. This makes them suitable for representing hierarchical or tree-like structures. You create a list with the function `list()`:

```{r}
x <- list(1, 2, 3)
x
```

A very useful tool for working with lists is `str()` because it focuses on displaying the **str**ucture, not the contents.

```{r}
str(x)
x_named <- list(a = 1, b = 2, c = 3)
str(x_named)
```

Unlike atomic vectors, `list()` can contain a mix of objects.

```{r}
y <- list("a", 1L, 1.5, TRUE)
str(y)
```

Lists can even contain other lists!

```{r}
z <- list(list(1, 2), list(3, 4))
str(z)
```

### Visualizing lists

To explain more complicated list manipulation functions, it's helpful to have a visual representation of lists. For example, take these three lists:

```{r}
x1 <- list(c(1, 2), c(3, 4))
x2 <- list(list(1, 2), list(3, 4))
x3 <- list(1, list(2, list(3)))
```

I'll draw them as follows:

```{r, echo = FALSE}
knitr::include_graphics("02-wrangling/images/lists-structure.png")
```

There are three principles:

1. Lists have rounded corners. Atomic vectors have square corners.

1. Children are drawn inside their parent, and have a slightly darker background to make it easier to see  the hierarchy.

1. The orientation of the children (i.e. rows or columns) isn't important, so I'll pick a row or column orientation to either save space or illustrate an important property in the example.

### Subsetting

There are three ways to subset a list, which I'll illustrate with a list named `a`:

```{r}
a <- list(a = 1:3, b = "a string", c = pi, d = list(-1, -5))
```

`[ ]` extracts a sub-list. The result will always be a list.

```{r}
str(a[1:2])
str(a[4])
```

Like with vectors, you can subset with a logical, integer, or character vector.
    
`[[ ]]` extracts a single component from a list. It removes a level of hierarchy from the list.

```{r}
str(a[[1]])
str(a[[4]])
```

`$` is a shorthand for extracting named elements of a list. It works similarly to `[[ ]]` except that you don't need to use quotes.
    
```{r}
a$a
a[["a"]]
```

The distinction between `[ ]` and `[[ ]]` is really important for lists, because `[[ ]]` drills down into the list while `[ ]` returns a new, smaller list. Compare the code and output above with the visual representation.

## Date-Times 

We will manipulate date-times using the **lubridate** package, which makes it easier to work with dates and times in R. **lubridate** is not part of the core tidyverse because you only need it when you're working with dates/times.

```{r echo = FALSE}
knitr::include_graphics("https://imgs.xkcd.com/comics/iso_8601.png")
```

There are three types of date/time data that refer to an instant in time:

* A `date`. Tibbles print this as `<date>`.

* A `time` within a day. Tibbles print this as `<time>`.

* A `date-time` is a date plus a time: it uniquely identifies an
  instant in time (typically to the nearest second). Tibbles print this
  as `<dttm>`.

You should always use the simplest possible datatype that works for your needs. That means if you can use a date instead of a date-time, you should. Date-times are substantially more complicated because of the need to handle time zones, which we'll come back to at the end of the chapter.

To get the current date or date-time you can use `today()` or `now()`:

```{r}
today()
now()
```

Otherwise, there are three ways you're likely to create a date/time:

1. From a string.
1. From individual date-time components.
1. From an existing date/time object.

They work as follows:

### From strings

Date/time data often comes as strings. The **lubridate** functions automatically work out the format once you specify the order of the component. To use them, identify the order in which year, month, and day appear in your dates, then arrange "y", "m", and "d" in the same order. That gives you the name of the **lubridate** function that will parse your date. For example:

```{r}
ymd("2017-01-31")
mdy("January 31st, 2017")
dmy("31-Jan-2017")
```

These functions also take unquoted numbers. This is the most concise way to create a single date/time object, as you might need when filtering date/time data. `ymd()` is short and unambiguous:

```{r}
ymd(20170131)
```

`ymd()` and friends create dates. To create a date-time, add an underscore and one or more of "h", "m", and "s" to the name of the parsing function:

```{r}
ymd_hms("2017-01-31 20:11:59")
mdy_hm("01/31/2017 08:01")
```

You can also force the creation of a date-time from a date by supplying a timezone:

```{r}
ymd(20170131, tz = "UTC")
```


### From individual components

Instead of a single string, sometimes you'll have the individual components of the date-time spread across multiple columns. This is what we have in the flights data:

```{r}
flights %>% 
  select(year, month, day, hour, minute)
```

To create a date/time from this sort of input, use `make_date()` for dates, or `make_datetime()` for date-times:

```{r}
flights %>% 
  select(year, month, day, hour, minute) %>% 
  mutate(departure = make_datetime(year, month, day, hour, minute))
```

### From other types

You may want to switch between a date-time and a date. That's the job of `as_datetime()` and `as_date()`:

```{r}
as_datetime(today())
as_date(now())
```

Sometimes you'll get date/times as numeric offsets from the "Unix Epoch", 1970-01-01. If the offset is in seconds, use `as_datetime()`; if it's in days, use `as_date()`.

```{r}
as_datetime(60 * 60 * 10)
as_date(365 * 10 + 2)
```

### Date-time components

Now that you know how to get date-time data into R's date-time data structures, let's explore what you can do with them. This section will focus on the accessor functions that let you get and set individual components. The next section will look at how arithmetic works with date-times.

You can pull out individual parts of the date with the accessor functions `year()`, `month()`, `mday()` (day of the month), `yday()` (day of the year), `wday()` (day of the week), `hour()`, `minute()`, and `second()`. 

```{r}
datetime <- ymd_hms("2016-07-08 12:34:56")
year(datetime)
month(datetime)
mday(datetime)
yday(datetime)
wday(datetime)
```

For `month()` and `wday()` you can set `label = TRUE` to return the abbreviated name of the month or day of the week. Set `abbr = FALSE` to return the full name.

```{r}
month(datetime, label = TRUE)
wday(datetime, label = TRUE, abbr = FALSE)
```


### Setting components

You can create a new date-time with `update()`.

```{r}
update(datetime, year = 2020, month = 2, mday = 2, hour = 2)
```

If values are too big, they will roll-over:

```{r}
ymd("2015-02-01") %>% 
  update(mday = 30)
ymd("2015-02-01") %>% 
  update(hour = 400)
```


### Time zones 

Time zones are an enormously complicated topic because of their interaction with geopolitical entities. Fortunately we don't need to dig into all the details as they're not all that important for data analysis. You can see the complete list of possible timezones with the function `OlsonNames()`. Unless otherwise specified, **lubridate** always uses UTC (Coordinated Universal Time).

In R, the time zone is an attribute of the date-time that only controls printing. For example, these three objects represent the same instant in time:

```{r}
(x1 <- ymd_hms("2015-06-01 12:00:00", tz = "America/New_York"))
(x2 <- ymd_hms("2015-06-01 18:00:00", tz = "Europe/Copenhagen"))
(x3 <- ymd_hms("2015-06-02 04:00:00", tz = "Pacific/Auckland"))
```

## Combining Data

There are many ways to bring data together. 

```{r, echo = FALSE, fig.cap = "Combining data is often tricky."}
knitr::include_graphics("02-wrangling/images/wrangling.gif")
```

The `bind_rows()` function is used to combine all the rows from two or more tibbles.


```{r}
data_1 <- tibble(x = 1:2,                
                 y = c("A", "B")) 

data_2 <- tibble(x = 3:4,
                 y = c("C", "D")) 


bind_rows(data_1, data_2)
```


### Joins


Consider two tibbles: `superheroes` and `publishers`.

```{r}

superheroes <- tibble::tribble(
       ~name,   ~gender,     ~publisher,
   "Magneto",   "male",       "Marvel",
     "Storm",   "female",     "Marvel",
    "Batman",   "male",       "DC",
  "Catwoman",   "female",     "DC",
   "Hellboy",   "male",       "Dark Horse Comics"
  )

publishers <- tibble::tribble(
  ~publisher, ~yr_founded,
        "DC",       1934L,
    "Marvel",       1939L,
     "Image",       1992L
  )
```

Note how easy it is to use `tribble()`, from the **tibble** package (part of the Tidyverse) to create a tibble on the fly using text which organized for easy entry and reading. Recall that a double colon --- `::` --- is how we indicate that a function comes from a specific package.

#### `inner_join()`

`inner_join(x, y)`: Return all rows from `x` where there are matching values in `y`, and all columns from `x` and `y`. If there are multiple matches between `x` and `y`, all combination of the matches are returned. 

```{r, echo= FALSE, fig.cap = "Inner join."}
knitr::include_graphics("02-wrangling/images/join-inner.png")
```

```{r}
inner_join(superheroes, publishers)
```

We lose Hellboy in the join because, although he appears in `x = superheroes`, his publisher Dark Horse Comics does not appear in `y = publishers`. The join result has all variables from `x = superheroes` plus `yr_founded`, from `y`.

Note the message that we are 'Joining, by = "publisher"'. Whenever joining, R checks to see if there are variables in common between the two tibbles and, if there are, uses them to join. However, it is concerned that you may not be aware that this is what it is doing, so R tells you. Such messages are both annoying and a signal that we have not made our code as robust as we should. Fortunately, we can specify precisely which variables we want to join by. Always do this.


```{r, eval = FALSE}
inner_join(superheroes, publishers, by = "publisher")
```

`by` also takes a vector of key variables if you want to merge by multiple variables.

Now compare this result to that of using `inner_join()` with the two datasets in opposite positions. 

```{r}
inner_join(publishers, superheroes, by = "publisher")
```

In a way, this does illustrate multiple matches, if you think about it from the `x = publishers` direction. Every publisher that has a match in `y = superheroes` appears multiple times in the result, once for each match. In fact, we're getting the same result as with `inner_join(superheroes, publishers)`, up to variable order (which you should also never rely on in an analysis).

#### `full_join()`

`full_join(x, y)`: Return all rows and all columns from both `x` and `y`. Where there are not matching values, returns `NA` for the one missing. 

```{r}
full_join(superheroes, publishers, by = "publisher")
```

We get all rows of `x = superheroes` plus a new row from `y = publishers`, containing the publisher Image. We get all variables from `x = superheroes` AND all variables from `y = publishers`. Any row that derives solely from one table or the other carries `NA`s in the variables found only in the other table.


Because `full_join()` returns all rows and all columns from both `x` and `y` the result of `full_join(x = superheroes, y = publishers)` should match that of `full_join(x = publishers, y = superheroes)`.

#### `left_join()`

`left_join(x, y)`: Return all rows from `x`, and all columns from `x` and `y`. If there are multiple matches between `x` and `y`, all combination of the matches are returned. 

```{r}
left_join(superheroes, publishers, by = "publisher")
```

We basically get `x = superheroes` back, but with the addition of variable `yr_founded`, which is unique to `y = publishers`. Hellboy, whose publisher does not appear in `y = publishers`, has an `NA` for `yr_founded`.

Now compare this result to that of running `left_join(x = publishers, y = superheroes)`. Unlike `inner_join()` and `full_join()` the order of the arguments has a significant effect on the resulting tibble. 

```{r}
left_join(publishers, superheroes, by = "publisher")
```

We get a similar result as with `inner_join()` but the publisher Image survives in the join, even though no superheroes from Image appear in `y = superheroes`. As a result, Image has `NA`s for `name` and `gender`.

There is a similar function, `right_join(x, y)` that return all rows from `y`, and all columns from `x` and `y`. 

#### `semi_join()`

`semi_join(x, y)`: Return all rows from `x` where there are matching values in `y`, keeping just columns from `x`. A semi join differs from an inner join because an inner join will return one row of `x` for each matching row of `y`, where a semi join will never duplicate rows of `x`. This is a filtering join.

```{r}
semi_join(superheroes, publishers, by = "publisher")
```

Compare the result of switching the values of the arguments. 

```{r}
semi_join(x = publishers, y = superheroes, by = "publisher")
```

Now the effects of switching the `x` and `y` roles is more clear. The result resembles `x = publishers`, but the publisher Image is lost, because there are no observations where `publisher == "Image"` in `y = superheroes`.

#### `anti_join()`

`anti_join(x, y)`: Return all rows from `x` where there are no matching values in `y`, keeping just columns from `x`. 

```{r}
anti_join(superheroes, publishers, by = "publisher")
```

We keep only Hellboy now (and do not get `yr_founded`).

Now switch the arguments and compare the result.

```{r}
anti_join(publishers, superheroes, by = "publisher")
```

We keep only publisher Image now (and the variables found in `x = publishers`).


### Example

Consider the relationships among the tibbles in the **nycflights** package:


```{r, echo = FALSE, fig.cap = "Relationships among nycflights tables"}
knitr::include_graphics("02-wrangling/images/relational-nycflights.png")
```


In both the `flights` and `airlines` data frames, the key variable we want to join/merge/match the rows by has the same name: `carrier`. Let's use `inner_join()` to join the two data frames, where the rows will be matched by the variable `carrier`

```{r}
flights %>% 
  inner_join(airlines, by = "carrier")
```

This is our first example of using a join function with a pipe. `flights` is fed as the first argument to `inner_join()`. That is what the pipe does. The above code is equivalent to:

```{r, eval = FALSE}
inner_join(flights, airlines, by = "carrier")
```

The `airports` data frame contains the airport codes for each airport:

```{r}
airports
```

However, if you look at both the `airports` and `flights` data frames, you'll find that the airport codes are in variables that have different names. In `airports` the airport code is in `faa`, whereas in `flights` the airport codes are in `origin` and `dest`. 

In order to join these two data frames by airport code, our `inner_join()` operation will use `by = c("dest" = "faa")` thereby allowing us to join two data frames where the key variable has a different name.

```{r}
flights %>% 
  inner_join(airports, by = c("dest" = "faa"))
```

Let's construct the chain of pipe operators `%>%` that computes the number of flights from NYC to each destination, but also includes information about each destination airport:

```{r}
flights %>%
  group_by(dest) %>%
  summarize(num_flights = n(),
            .groups = "drop") %>%
  arrange(desc(num_flights)) %>%
  inner_join(airports, by = c("dest" = "faa")) %>%
  rename(airport_name = name)
```

`"ORD"` is the airport code of Chicago O'Hare airport and `"FLL"` is the code for the main airport in Fort Lauderdale, Florida, which can be seen in the `airport_name` variable.



## Tidy data


Consider the first five rows from the `drinks` data frame from the **fivethirtyeight** package:

```{r, echo = FALSE}
drinks %>%  
  head(5)
```

After reading the help file by running `?drinks`, you'll see that `drinks` is a data frame containing results from a survey of the average number of servings of beer, spirits, and wine consumed in 193 countries. This data was originally reported on FiveThirtyEight.com in Mona Chalabi's article: ["Dear Mona Followup: Where Do People Drink The Most Beer, Wine And Spirits?"](https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/).

Let's apply some of the data wrangling verbs on the `drinks` data frame:

1. `filter()` the `drinks` data frame to only consider 4 countries: the United States, China, Italy, and Saudi Arabia, *then*
1. `select()` all columns except `total_litres_of_pure_alcohol` by using the `-` sign, *then*
1. `rename()` the variables `beer_servings`, `spirit_servings`, and `wine_servings` to `beer`, `spirit`, and `wine`, respectively.

We will save the resulting data frame in `drinks_smaller`.

```{r}
drinks_smaller <- drinks %>%
  filter(country %in% c("USA", "China", "Italy", "Saudi Arabia")) %>%
  select(-total_litres_of_pure_alcohol) %>%
  rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings)
drinks_smaller
```


### Definition of "tidy" data 

What does it mean for your data to be "tidy"? While "tidy" has a clear English meaning of "organized," the word "tidy" in data science using R means that your data follows a standardized format. 

"Tidy" data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In *tidy data*:

1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.


```{r, echo = FALSE}
knitr::include_graphics("02-wrangling/images/tidy-1.png")
```

### Converting to "tidy" data

In this book so far, you've only seen data frames that were already in "tidy" format. Furthermore, for the rest of this book, you'll mostly only see data frames that are already in "tidy" format as well. This is not always the case however with all datasets in the world. If your original data frame is in wide (non-"tidy") format and you would like to use the **ggplot2** or **dplyr** packages, you will first have to convert it to "tidy" format. To do so, we recommend using the `pivot_longer()` function in the **tidyr** package [@R-tidyr]. 

Going back to our `drinks_smaller` data frame from earlier:

```{r}
drinks_smaller
```

We convert it to "tidy" format by using the `pivot_longer()` function from the **tidyr** package as follows:

```{r}
drinks_smaller_tidy <- drinks_smaller %>% 
  pivot_longer(names_to = "type", 
               values_to = "servings", 
               cols = -country)
drinks_smaller_tidy
```

Let's dissect the arguments to `pivot_longer()`.

1. the first argument`names_to` corresponds to the name of the variable in the new "tidy"/long data frame that will contain the *column names* of the original data. Observe how we set `names_to = "type"`. In the resulting `drinks_smaller_tidy`, the column `type` contains the three types of alcohol `beer`, `spirit`, and `wine`. Since `type` is a variable name that doesn't appear in `drinks_smaller`, we use quotation marks around it. You'll receive an error if you just use `names_to = type` here.

1. The second argument `values_to` corresponds to the name of the variable in the new "tidy" data frame that will contain the *values* of the original data. Observe how we set `values_to = "servings"` since each of the numeric values in each of the `beer`, `wine`, and `spirit` columns of the `drinks_smaller` data corresponds to a value of `servings`. In the resulting `drinks_smaller_tidy`, the column `servings` contains the 4 $\times$ 3 = 12 numerical values. Note again that `servings` doesn't appear as a variable in `drinks_smaller` so it again needs quotation marks around it for the `values_to` argument.

1. The third argument `cols` is the columns in the `drinks_smaller` data frame you either want to or don't want to "tidy." Observe how we set this to `-country` indicating that we don't want to "tidy" the `country` variable in `drinks_smaller` and rather only `beer`, `spirit`, and `wine`. Since `country` is a column that appears in `drinks_smaller` we don't put quotation marks around it.

The third argument here of `cols` is a little nuanced, so let's consider code that's written slightly differently but that produces the same output: 

```{r, eval=FALSE}
drinks_smaller %>% 
  pivot_longer(names_to = "type", 
               values_to = "servings", 
               cols = c(beer, spirit, wine))
```

Note that the third argument now specifies which columns we want to "tidy" with `c(beer, spirit, wine)`, instead of the columns we don't want to "tidy" using `-country`. We use the `c()` function to create a vector of the columns in `drinks_smaller` that we'd like to "tidy." Note that since these three columns appear one after another in the `drinks_smaller` data frame, we could also do the following for the `cols` argument:

```{r, eval = FALSE}
drinks_smaller %>% 
  pivot_longer(names_to = "type", 
               values_to = "servings", 
               cols = beer:wine)
```


Converting "wide" format data to "tidy" format often confuses new R users. The only way to learn to get comfortable with the `pivot_longer()` function is with practice, practice, and more practice using different datasets. For example, run `?pivot_longer` and look at the examples in the bottom of the help file.

If however you want to convert a "tidy" data frame to "wide" format, you will need to use the `pivot_wider()` function instead. Run `?pivot_wider` and look at the examples in the bottom of the help file for examples.

You can also view examples of both `pivot_longer()` and `pivot_wider()` on the [tidyverse.org](https://tidyr.tidyverse.org/dev/articles/pivot.html#pew) webpage. There's a nice example to check out the different functions available for data tidying and a case study using data from the World Health Organization on that webpage. Furthermore, each week the R4DS Online Learning Community posts a dataset in the weekly [`#`TidyTuesday event](https://github.com/rfordatascience/tidytuesday) that might serve as a nice place for you to find other data to explore and transform. 



## Distributions {#distributions}

<!-- DK: This could be re-organized. More discusion of discrete versus continuous. -->

What is a distribution? *Think of the distribution of a variable as an urn from which we can pull out, at random, values for that variable.* Drawing a thousand or so values from that urn, and then looking at a histogram, can show where the values are centered and how the values vary. Because we are sloppy, we will use the word distribution to refer to both the (imaginary!) urn from which we are drawing values and to the list of values we have drawn. *The vector of `r length(trains$age)` ages in the `trains` data is a distribution.*

### Scaling a distribution

Consider the vector which is the result of rolling one die 10 times.

```{r}
rolls <- c(5, 5, 1, 5, 4, 2, 6, 2, 1, 5)
```

There are other ways of storing the data in this vector. Instead of recording every draw, we could just record the number of times each value appears.

```{r}
table(rolls)
```

In this case, with only 10 vlaues, it is actually less efficient to store the data like this. But what happens when we have 10,000 rolls.

```{r}
more_rolls <- rep(rolls, 1000)
table(more_rolls)
```

Instead of keeping around a vector of length 10,000, we can just keep 10 values, without losing any information.

This example also highlights the fact that, graphically, two distributions can be identical even if they are of very different lengths.

```{r, echo = FALSE}
rolls_p <- tibble(value = rolls) %>% 
  ggplot(aes(value)) +
    geom_bar(aes(y = after_stat(count))) +
    labs(title = "Distribution of 10 Rolls",
         y = "Count")


more_rolls_p <- tibble(value = more_rolls) %>% 
  ggplot(aes(value)) +
    geom_bar(aes(y = after_stat(count))) +
    labs(title = "Distribution of 10,000 Rolls",
         y = NULL)


rolls_p + more_rolls_p
```

The two vectors --- `rolls` and `more_rolls` --- have the exact same shape because, even though their lengths differ, they are the same thing. *The total count for each value does not matter. What matters is the relative proportions.*

Since shape is what matters, we will often "normalize" distributions so that the sum of the counts equals one, meaning that the y-axis is a percentage of the total. Example:

```{r, echo = FALSE}
rolls_p <- tibble(value = rolls) %>% 
  ggplot(aes(x = value)) +
    geom_bar(aes(y = after_stat(count/sum(count)))) +
    labs(title = "Distribution of 10 Rolls",
         y = "Percentage")

more_rolls_p <- tibble(value = more_rolls) %>% 
  ggplot(aes(value)) +
    geom_bar(aes(y = after_stat(count/sum(count)))) +
    labs(title = "Distribution of 10,000 Rolls",
         y = NULL)

rolls_p + more_rolls_p
```

### `sample()`

The most common distributions you will work with are *empirical* or *frequency* distributions, the values of `age` in the `trains` tibble, the values of `poverty` in the `kenya` tibble, and so on. But we can also create our own data by making "draws" from a distribution which we have concocted. 

Consider the distribution of the possible values from rolling a fair die. We can use the `sample()` function to create draws from this distribution, meaning it will change (or sometimes stay the same) for every subsequent draw.

```{r}
die <- c(1, 2, 3, 4, 5, 6)

sample(x = die, size = 1)
```

This produces one "draw" from the distribution of the possible values of one roll of fair six-sided die.

Now, suppose we wanted to roll this die 10 times. One of the arguments of the `sample()` function is `replace`. We must specify it as `TRUE` if values can appear more than once. Since, when rolling a die 10 times, we expect that a value like 3 can appear more than once, we need to set `replace = TRUE`. 

```{r}
sample(x = die, size = 10, replace = TRUE)
```

In other words, rolling a 1 on the first roll should not preclude you from rolling a 1 on a later roll. 

What if the die is not "fair," meaning that some sides are more likely to appear then others? The final argument of the `sample()` function is the `prob` argument. It takes a vector (of the same length as the initial vector `x`) that contains all of the probabilities of selecting each one of the elements of `x`. Suppose that the probability of rolling a 1 was 0.5, and the probability of rolling any other value is 0.1. (These probabilities should sum to 1. If they don't `sample()` will automatically re-scale them.) 

```{r}
sample(x = die, 
       size = 10, 
       replace = TRUE, 
       prob = c(0.5, 0.1, 0.1, 0.1, 0.1, 0.1))
```

**Remember: There is no real data here.** We have not actually rolled a die. We have just made some assumptions about what would happen if we were to roll a die. With those assumptions we have built an urn --- a *data generating mechanism* --- from which we can draw as many values as we like. Let's roll the unfair die 10,000 times.

```{r}
tibble(result = sample(x = die, 
                       size = 10000, 
                       replace = TRUE, 
                       prob = c(0.5, rep(0.1, 5)))) %>% 
  ggplot(aes(x = result)) +
    geom_bar() +
    labs(title = "Distribution of Results of an Unfair Die",
         x = "Result of One Roll",
         y = "Count") +
    scale_x_continuous(breaks = 1:6,
                       labels = as.character(1:6)) +
    scale_y_continuous(labels = scales::comma_format())
```

This makes the *dual nature of distributions* more clear. A distribution is the "thing" you see in this plot. A mathematical object with several different parts, including a set of possible values (1 through 6, in this case) and a record of the number of times each appears. A distribution is *also* the simple vector of numbers we used to create this plot.

In general, we travel back-and-forth between distribution as a thing and distribution as a vector of numbers depending on what we are trying to accomplish. The path back-and-force is via draws.

`sample()` is just one of many functions for creating draws --- or, more colloquially, "drawing" --- from a distribution. Three of the most important functions are: `runif()`, `rbinom()`, and `rnorm()`.

### `runif()`

Consider a "uniform" distribution. This is the case in which every outcome in the range of possible outcomes has the same chance of occurring.  The function `runif()` (spoken as "r-unif") enables us to draw from a uniform contribution. `runif()` has three arguments: `n`, `min`, and `max`. `runif()` will produce `n` draws from between `min` and `max`, with each value having an equal chance of occurring.

```{r}
runif(n = 10, min = 4, max = 6)
```

Mathematically, we would write:

$$y_i \sim U(4, 6)$$, 

This means that the each value for $y$ is drawn from a uniform distribution between four and six.

### `rbinom()`

Consider binomial distribution, the case in which the probability of some Boolean variable (for instance success or failure) is calculated for repeated, independent trials. One common example would be the probability of flipping a coin and landing on heads. The function `rbinom()` allows us to draw from a binomial distribution. This function takes three arguments, `n`, `size`, and `prob`. 

*`n` is the number of values we seek to draw. 
*`size` is the number of trials for each `n`. 
*`prob` is the probability of success on each trial. 

Suppose we wanted to flip a fair coin one time, and let landing on heads represent success. 

```{r}
rbinom(n = 1 , size = 1, prob = 0.5)
```

Do the same thing 100 times:

```{r}
tibble(heads = rbinom(n = 100, size = 1, prob = 0.5)) %>% 
  ggplot(aes(x = heads)) +
    geom_bar() +
    labs(title = "Flipping a Fair Coin 100 Times",
         x = "Result",
         y = "Count") +
    scale_x_continuous(breaks = c(0, 1),
                       labels = c("Tails", "Heads"))
```

In our graph above, we use the function `scale_x_continuous()` because our x-axis variable is continuous, meaning it can take on any real values. The `breaks` argument to `scale_x_continuous()` converts our x-axis to having two different "tick marks". There is a fairly even distribution of Tails and Heads. More draws would typically result in an even more equal split. 

Randomness creates (inevitable) tension between distribution as a "thing" and distribution as a vector of draws from that thing. In this case, the vector of draws is not balanced between Tails and Heads. Yet, we "know" that it should be since the coin is, by definition, fair. In a sense, the mathematics require an even split. Yet, randomness means that the vector of draws will rarely match the mathematically "true" result. And that is OK! First, randomness is an intrinsic property of the real world. Second, we can make the effect of randomness be as small as we want by increasing the number of draws. 

<!-- DK: Could do a little example of that, showing the convergence to 50% as n increaeses. -->


Suppose instead we wanted to simulate an unfair coin, where the probability of landing on Heads was 0.75 instead of 0.25. 

```{r}
tibble(heads = rbinom(n = 100, size = 1, prob = 0.75)) %>% 
  ggplot(aes(x = heads)) +
    geom_bar() +
    labs(title = "Flipping a Fair Coin 100 Times",
         x = "Result",
         y = "Count") +
    scale_x_continuous(breaks = c(0, 1),
                       labels = c("Tails", "Heads"))
```

The distribution --- the imaginary urn --- from which we draw the results of a coin flip for a fair coin is a different distribution --- a different imaginary urn --- from the distribution for a biased coin. In fact, there are an infinite number of distributions. *Yet as long as we can draw values from a distribution, we can work with it.* Mathematics:

$$y_i \sim B(n, p)$$. 

Each value for $y$ is drawn from a binomial distribution with parameters $n$ for the number of trials and $p$ for the probability of success. 

Instead of each `n` consisting of a single trial, we could have situation in which we are, 10,000 times, flipping a coin 10 times and summing up, for each experiment, the number of heads. In that case:

```{r}
set.seed(9)
tibble(heads = rbinom(n = 10000, size = 10, prob = 0.5)) %>% 
  ggplot(aes(x = heads)) +
    geom_bar() +
    labs(title = "Flipping a Fair Coin 10 Times",
         subtitle = "Extreme results are possible with enough experiments",
         x = "Total Number of Heads in Ten Flips",
         y = "Count") +
    scale_x_continuous(breaks = 0:10)
```


### `rnorm()` {#normal}

<!-- DK: Given that we have a cool plot at the end, maybe we don't need these earlier plots.  Or maybe we need plots for each kind of distribution. -->

<!-- DK: Need to discuss the fact that we use y_i each time. -->

The most important distribution is the *normal distribution*. Mathematics:

$$y_i \sim N(\mu, \sigma^2)$$. 

Each value $y_i$ is drawn from a normal distribution with parameters $\mu$ for the mean and $\sigma$ for the standard deviation. 


This bell-shaped distribution is defined by two parameters: (1) the *mean* $\mu$ (spoken as "mu") which locates the center of the distribution and (2) the *standard deviation* $\sigma$ (spoken as "sigma") which determines the variation of values around that center. In the figure below, we plot three normal distributions where:

1. The solid normal curve has mean $\mu = 5$ \& standard deviation $\sigma = 2$.
1. The dotted normal curve has mean $\mu = 5$ \& standard deviation $\sigma = 5$.
1. The dashed normal curve has mean $\mu = 15$ \& standard deviation $\sigma = 2$.

```{r, echo = FALSE, fig.cap = "Three normal distributions."}
library(ggrepel)
all_points <- tibble(
  domain = seq(from = -10, to = 25, by = 0.01),
  `mu = 5, sigma = 2` = dnorm(x = domain, mean = 5, sd = 2),
  `mu = 5, sigma = 5` = dnorm(x = domain, mean = 5, sd = 5),
  `mu = 15, sigma = 2` = dnorm(x = domain, mean = 15, sd = 2)
)  %>% 
  pivot_longer(- domain, names_to = "Distribution", values_to = "value", ) %>% 
  mutate(
    Distribution = factor(
      Distribution, 
      levels = c("mu = 5, sigma = 2", 
                 "mu = 5, sigma = 5", 
                 "mu = 15, sigma = 2")
    )
  )
for_labels <- all_points %>% 
  filter(between(domain, 3.795, 3.805) & Distribution == "mu = 5, sigma = 2" |
           between(domain, 0.005, 0.0105) & Distribution == "mu = 5, sigma = 5" |
           between(domain, 16.005, 16.015) & Distribution == "mu = 15, sigma = 2")
all_points %>% 
  ggplot(aes(x = domain, y = value, linetype = Distribution)) +
  geom_line() +
  geom_label_repel(data = for_labels, aes(label = Distribution),
                            nudge_x = c(-1, -2.1, 1)) +
  theme_light() +
  scale_linetype_manual(values=c("solid", "dotted", "longdash")) + 
  theme(
    axis.title.y = element_blank(),
    axis.title.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    legend.position = "none"
  )
```

Notice how the solid and dotted line normal curves have the same center due to their common mean $\mu$ = 5. However, the dotted line normal curve is wider due to its larger standard deviation of $\sigma = 5$. On the other hand, the solid and dashed line normal curves have the same variation due to their common standard deviation $\sigma = 2$. However, they are centered at different locations. 

When the mean $\mu = 0$ and the standard deviation $\sigma = 1$, the normal distribution has a special name. It's called the *standard normal distribution* or the *$z$-curve*.

Furthermore, if a variable follows a normal curve, there are *three rules of thumb* we can use:

1. 68% of values will lie within $\pm$ 1 standard deviation of the mean.
1. 95% of values will lie within $\pm$ 1.96 $\approx$ 2 standard deviations of the mean.
1. 99.7% of values will lie within $\pm$ 3 standard deviations of the mean.

Let's illustrate this on a standard normal curve. The dashed lines are at -3, -1.96, -1, 0, 1, 1.96, and 3. These 7 lines cut up the x-axis into 8 segments. The areas under the normal curve for each of the 8 segments are marked and add up to 100%. For example:

1. The middle two segments represent the interval -1 to 1. The shaded area above this interval represents 34% + 34% = 68% of the area under the curve. In other words, 68% of values. 
1. The middle four segments represent the interval -1.96 to 1.96. The shaded area above this interval represents 13.5% + 34% + 34% + 13.5% = 95% of the area under the curve. In other words, 95% of values. 
1. The middle six segments represent the interval -3 to 3. The shaded area above this interval represents 2.35% + 13.5% + 34% + 34% + 13.5% + 2.35% = 99.7% of the area under the curve. In other words, 99.7% of values. 

```{r, echo = FALSE, fig.cap = "Rules of thumb about areas under normal curves."}
shade_3_sd <- function(x) {
  y <- dnorm(x, mean = 0, sd = 1)
  y[x <= -3 | x >= 3] <- NA
  return(y)
}
shade_2_sd <- function(x) {
  y <- dnorm(x, mean = 0, sd = 1)
  y[x <= -1.96 | x >= 1.96] <- NA
  return(y)
}
shade_1_sd <- function(x) {
  y <- dnorm(x, mean = 0, sd = 1)
  y[x <= -1 | x >= 1] <- NA
  return(y)
}
labels <- tibble(
  x = c(-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5),
  label = c("0.15%", "2.35%", "13.5%", "34%", "34%", "13.5%", "2.35%", "0.15%")
) %>% 
  mutate(y = rep(0.3, times = n()))
ggplot(data = tibble(x = c(-4, 4)), aes(x)) +
  geom_text(data = labels, aes(y=y, label = label)) + 
  # Trace normal curve
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), n = 1000) + 
  # Shade and delineate +/- 3 SD
  stat_function(fun = shade_3_sd, geom = "area", fill = "black", alpha = 0.25, n = 1000) +
  # annotate(geom = "segment", x = c(3, -3), xend = c(3, -3), y = 0, yend = dnorm(3, mean = 0, sd = 1)) +
  # Shade and delineate +/- 2 SD
  stat_function(fun = shade_2_sd, geom = "area", fill = "black", alpha = 0.25, n = 1000) +
  # annotate(geom = "segment", x = c(1.96, -1.96), xend = c(1.96, -1.96), y = 0, yend = dnorm(1.96, mean = 0, sd = 1)) +
  # Shade and delineate +/- 1 SD
  stat_function(fun = shade_1_sd, geom = "area", fill = "black", alpha = 0.25, n = 1000) +
  # annotate(geom = "segment", x = c(1, -1), xend = c(1, -1), y = 0, yend = dnorm(1, mean = 0, sd = 1)) + 
  geom_vline(xintercept = c(-3, -1.96, -1, 0, 1, 1.96, 3), linetype = "dashed", alpha = 0.5) +
  # Axes
  scale_x_continuous(breaks = seq(from = -3, to = 3, by = 1)) +
  labs(x = "z", y = "") +
  theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank())
```




```{r, echo = FALSE}
set.seed(5)
```

The function `rnorm()` (spoken as "r-norm") returns draws from a normal distribution. `rnorm()` has three arguments: `n`, `mean`, and `sd`. `n` corresponds to the number of draws, `mean` and `sd` are the $\mu$ and $\sigma$ of the distribution from which we want to draw.  Again, imagine an urn filled with beads. Each bead has a number written on it. If the distribution is standard normal, then we can draw 10 beads from the urn by running: 

```{r}
rnorm(10)
```

These 10 draws come from a distribution with the default `mean` of 0 and the default `sd` of 1. What if we create a histogram of the values?

```{r}
tibble(value = rnorm(10)) %>% 
  ggplot(aes(x = value)) + 
    geom_histogram(bins = 10)
```

As you can see, it is not as symmetrical as the one displayed above. This is not surprising! If you just draw 10 beads from the urn, you can not possibly have a very good sense of what all the numbers on all the beads in the urn look like. What if we draw 100 values? What about 100,000?

```{r}
tibble(value = rnorm(100)) %>% 
  ggplot(aes(x = value)) +
    geom_histogram(bins = 10)
```

```{r}
tibble(value = rnorm(100000)) %>% 
  ggplot(aes(x = value)) +
    geom_histogram(bins = 1000)
```

Now it's looking a lot more similar to the "truth", although still  imperfect. 

Now, let's compare normal distributions with varying means and standard deviations, which can be set using the mean and sd arguments included with the function. 

```{r}
tibble(rnorm_5_1 = rnorm(n = 1000, mean = 5, sd = 1), 
       rnorm_0_3 = rnorm(n = 1000, mean = 0, sd = 3),
       rnorm_0_1 = rnorm(n = 1000, mean = 0, sd = 1)) %>%
  pivot_longer(cols = everything(), 
               names_to = "distribution", 
               values_to = "value") %>% 
  ggplot(aes(x = value, fill = distribution)) +
    geom_density(alpha = 0.5) +
    labs(title = "Comparison of Normal Distributions with Differing Mean and Standard Deviation Values", 
         fill = "Distribution",
         x = "Value",
         y = "Density")
```




### Working with draws

<!-- DK: Should I be working with tibbles instead of vectors? -->

Once we have a vector of draws, we can examine various aspects of the distribution. Examples:

```{r}
draws <- rnorm(100, mean = 2, sd = 1)
```

Even though we know, because we wrote the code, that the draws come from a normal distribution with a mean of 2 and a standard deviation of 1, the calculated results will not match those values exactly because the draws themselves are random.

```{r}
mean(draws)
sd(draws)
```


Note that we are more likely to use the median and the mad to summarize a distribution. In this case, they are very similar to the mean and standard deviation.

```{r}
median(draws)
mad(draws)
```


In practice, we will not know the exact distribution which generates our data. (If we did know, then estimation would be unnecessary.) The inherent randomness of the world means that calculated statistics will not match the underlying truth perfectly. But the more data that we collect, the closer the match will be.

In addition to the mean and standard deviation of the draws, we will often be interested in various quantiles of the distribution, most commonly because we want to create intervals which cover a specified portion of the draws. Examples:

```{r}
quantile(draws, probs = c(0.25, 0.75))
quantile(draws, probs = c(0.05, 0.95))
quantile(draws, probs = c(0.025, 0.975))
```

Note that these draws come from a distribution which is centered around 2 rather than 0. There is nothing intrinsically special about any of these ranges. They are mere convention, especially the 95% interval.

<!-- DK: Discuss confidence intervals. -->

Note how cavalier we are in sometimes using the word "distribution" and sometimes the word "draws." These are two different things! The distribution is the underlying reality, which we will only know for certain when we create it ourselves, as in this example. The draws are a vector of numbers which, we assume, are "drawn" from some underlying distribution which, in general, we do not know. 

By assumption, we can analyze the draws to make inferences about the distribution.

Although distributions (and the draws therefrom) are complex, we can often treat them in the same way that we treat simple numbers. For example, we can add two distributions together.

```{r}
n <- 100000
tibble(Normal = rnorm(n, mean = 1),
       Uniform = runif(n, min = 2, max = 3),
       Combined = Normal + Uniform) %>% 
  pivot_longer(cols = everything(),
               names_to = "Distribution",
               values_to = "draw") %>% 
  ggplot(aes(x = draw, fill = Distribution)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   alpha = 0.5, 
                   bins = 100, 
                   position = "identity") +
    labs(title = "Two Distributions and Their Sum",
         subtitle = "You can sum distributions just like you sum numbers",
         x = "Value",
         y = "Probability")
```

Drawing from a distribution also allows us to answer questions via *simulation.* For example, imagine that A and B are both flipping fair coints. A flips the coin 3 times. B flips the coin 6 times. What is the probability that A flips more heads than B?

It is obvious that B will win this game more often than A. It is also obvious that A will win some of the  time. But in order to estimate the chances of A winning, we can simply simulate playing the game 1,000 times.


```{r}
set.seed(56)
games <- 1000 

tibble(A_heads = rbinom(n = games, size = 3, prob = 0.5),
       B_heads = rbinom(n = games, size = 6, prob = 0.5)) %>% 
  mutate(A_wins = ifelse(A_heads > B_heads, 1, 0)) %>% 
  summarize(A_chances = mean(A_wins))
```

A has about a 9% chance of winning the game.

<!-- DK: Could do more here, like look at prediction games, betting and so on. -->

## Other Commands


<!-- Look at the code used in the Temperance section of chapter 7. Explain all the key parts here, including rowwise, ungroup and matrices. Also, see the Preliminaries parts of the associated tutorials.  -->

<!-- Are these commands worth covering? -->

<!-- tidyr::separate_rows() -->
<!-- dplyr::coalesce() -->
<!-- expand(data, ...) -->
<!-- crossing(...) -->
<!-- nesting(...) -->

<!-- - dplyr's function (cut?) to turn continuous data into categorical data. expand() to see up data frames for creating all possible combinations? -->


Here are some topics which will prove important later in the *Primer*.

### Matrices

Recall that a "matrix" in R is a rectangular array of data, shaped like a data frame or tibble, but containing only one type of data, e.g., numeric. Large matrices also print out ugly. (There are other differences, none of which we care about here.) Example:

```{r}
m <- matrix(c(3, 4, 8, 9, 12, 13, 0, 15, -1), ncol = 3)
m
```

The easiest way to pull information from a matrix is to use `[ ]`, the subset operator. Here is how we grab the second and third columns of `m`:

```{r}
m[, 2:3]
```

Note, however, that matrices with just one dimension "collapse" into single vectors:

```{r}
m[, 2]
```

Tibbles, on the other hand, always maintain their rectangular shapes, even with only one column or row. 

We can turn matrices into tibbles with `as_tibble()`.

```{r}
m %>% 
  as_tibble()
```

Because `m` does not have column names, `as_tibble()` creates its won variables names, using "V" for variable.

### Missing Values 

Some observations in a tibble are blank. These are called missing values, and they are often marked as `NA`.  We can create such a tibble as follows:

```{r}
tbl <- tribble(
  ~ a, ~ b, ~ c,
    2,   3,   5,
    4,  NA,   8,
   NA,   7,   9,
)

tbl
```

The presence of `NA` values can be problematic.

```{r}
tbl %>% 
  summarize(avg_a = mean(a))
```

Fortunately, most R functions take an argument, `na.rm`, which, when set to `TRUE`, removes NA values from any calculations. 

```{r}
tbl %>% 
  summarize(avg_a = mean(a, na.rm = TRUE))
```

Another approach is to use `drop_na()`.

```{r}
tbl %>% 
  drop_na(a)
```

Be careful, however, if you use `drop_na()` without a specific variable provided. In that case, you will remove all rows with a missing value for any varible in the tibble.

```{r}
tbl %>% 
  drop_na()
```

A final approach is to use `is.na()` to explicitly determine if a value is missing.

```{r}
tbl %>% 
  mutate(a_missing = is.na(a))
```

### Working by rows

Tibbles and the main Tidyverse functions are designed to work by columns. You do something to all the values for variable `a`. But, sometimes, we want to work `across` the tibble, comparing the value for `a` in the first row to the value of `b` in the first row, and so on. To do that, we need two tricks. First, we use `rowwise()` to inform R that the next set of commands should executed across the rows.

```{r}
tbl %>% 
  rowwise()
```

Note how "# Rowwise: " is printed out. Having set up the pipe to work across rows, we need to pass `c_across()` to whichever function we are using, specifying the variables we want.

```{r}
tbl %>% 
  rowwise() %>% 
  mutate(largest = max(c_across())) %>% 
  mutate(largest_na = max(c_across(), na.rm = TRUE))
```



## Summary

This chapter covered many, many commands. You should have them all memorized by now.

No! That is ridiculous. We don't have them all memorized. Why should you? The point of this chapter was to give you a tour of what you can do in R and how to do it. With that information, you have a base from which to try to solve the problems you will encounter in the future.

<!-- DK: I am eliding over distributions of numbers (easy) and of values, like race. Should go back and have binom return "Head". Or maybe this is subtle enough that I can ignore it. Also, should make clear that sample() is, conceptually, taking draws from trains$age. Or, rather, trains$age is some draw from an imaginary distribution or population. -->

The key data science concept from this chapter is, again, the idea of a "distribution." The word distribution is used in two very different ways. First, a distribution is an invisible object that you can never use of touch. It is the imaginary urn from which you can take draws. Only in very special cases will you ever be able to "know" what the distribution is, mainly the case where there is a physical process, like a roulette wheel, which you can inspect or in the case of an assumed mathematical formula. But, in almost all real world data science problems, the "distribution" is a mental creation whose reality you can never confirm.

The second way that the word distribution is used to refer to a vector of values, a variable in an R tibble. The 115 ages in `trains` are a distribution as are 1,000 draws from `rnorm()`. 

Whether "distribution" means the imaginary object or a vector of numbers drawn from that imaginary object depends on context.

